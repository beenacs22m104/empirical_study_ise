{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec32ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from xml.etree.ElementTree import iterparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1bf799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import paths, read, save\n",
    "from utils.consts import n_procs\n",
    "\n",
    "post_types = {\"questions\": 1, \"answers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325d45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_gamedev_se():\n",
    "    \"\"\"Runs the bash script to unpack data from the 7z file\n",
    "    and creates a Posts.7z and a PostLinks.7z file.\n",
    "    \"\"\"\n",
    "    print(\"- Unpacking Game Dev. Stack Exchange data\")\n",
    "    data_dir = paths.raw_dir(\"gamedev_se\")\n",
    "    # redirects 7z output to keep terminal clean\n",
    "    os.system(f\"cd {data_dir} ; bash unpack_gamedev_se.sh > /dev/null\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a094cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_posts(ds):\n",
    "    \"\"\"Decompresses the Posts.7z archive for a given dataset\"\"\"\n",
    "    print(\"-- Decompressing Posts.7z\")\n",
    "    data_dir = paths.raw_dir(ds)\n",
    "    file = data_dir / \"Posts.7z\"\n",
    "    # redirects 7z output to keep terminal clean\n",
    "    os.system(f\"7z x {file} -o{data_dir} -y > /dev/null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268509c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_links(ds):\n",
    "    \"\"\"Decompresses the PostLinks.7z archive for a given dataset\"\"\"\n",
    "    print(\"-- Decompressing PostLinks.7z\")\n",
    "    data_dir = paths.raw_dir(ds)\n",
    "    file = data_dir / \"PostLinks.7z\"\n",
    "    # redirects 7z output to keep terminal clean\n",
    "    os.system(f\"7z x {file} -o{data_dir} -y > /dev/null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25aa6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_post_type(ds, post_type):\n",
    "    \"\"\"Selects posts from a given type from Posts.xml for a given dataset\n",
    "    Questions = post_type == 1\n",
    "    Answers = post_type == 2\n",
    "    \"\"\"\n",
    "    extract_posts(ds)\n",
    "    posts_path = paths.posts_xml(ds)\n",
    "\n",
    "    if post_type == post_types[\"questions\"]:\n",
    "        print(\"-- Selecting questions from Posts.xml\")\n",
    "        path = paths.questions_xml(ds)\n",
    "    else:\n",
    "        print(\"-- Selecting answers from Posts.xml\")\n",
    "        path = paths.answers_xml(ds)\n",
    "\n",
    "    # select using grep for performance\n",
    "    os.system(f\"grep -F 'PostTypeId=\\\"{post_type}\\\"' {posts_path} > {path}\")\n",
    "    # deleted Posts.xml to save space\n",
    "    # saves space but has to decompress the 7z archive twice\n",
    "    posts_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78beb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_xml_file(ds, post_type):\n",
    "    \"\"\"Splits a XML file into chunks of 1MM lines to allow\n",
    "    multiprocessing and to limit memory usage.\n",
    "    \"\"\"\n",
    "    data_dir = paths.raw_dir(ds)\n",
    "\n",
    "    if post_type == post_types[\"questions\"]:\n",
    "        print(\"-- Splitting questions into chunks\")\n",
    "        file_name = \"questions\"\n",
    "        path = paths.questions_xml(ds)\n",
    "    else:\n",
    "        print(\"-- Splitting answers into chunks\")\n",
    "        file_name = \"answers\"\n",
    "        path = paths.answers_xml(ds)\n",
    "\n",
    "    # reduce number of lines to save memory during parsing\n",
    "    os.system(f\"split -l 1000000 {path} {data_dir}/{file_name}_\")\n",
    "    path.unlink()  # Remove original XML file to save space\n",
    "\n",
    "    splits = list(paths.raw_dir(ds).glob(f\"{file_name}_*\"))\n",
    "\n",
    "    print(f\"-- {len(splits)} splits\")\n",
    "\n",
    "    # wrap file in tags for proper XML syntax\n",
    "    for path in splits:\n",
    "        os.system(f\"sed -i '1s/^/<posts>\\\\n/' {path}\")  # top tag\n",
    "        os.system(f\"echo '</posts>' >> {path}\")  # bottom tag\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4788d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting XML data\n",
      "- Unpacking Game Dev. Stack Exchange data\n",
      "Extracting data from gamedev_se\n",
      "- Extracting questions\n",
      "-- Decompressing Posts.7z\n",
      "-- Selecting questions from Posts.xml\n",
      "-- Splitting questions into chunks\n",
      "-- 1 splits\n",
      "-- Parsing questions\n",
      "--- Worker 0 started\n",
      "- Extracting duplicate pairs\n",
      "-- Decompressing PostLinks.7z\n",
      "-- Parsing duplicate pairs\n",
      "- Extracting answers\n",
      "-- Decompressing Posts.7z\n",
      "-- Selecting answers from Posts.xml\n",
      "-- Splitting answers into chunks\n",
      "-- 1 splits\n",
      "-- Parsing answers\n",
      "--- Worker 0 started\n",
      "Extracting data from stackoverflow\n",
      "- Extracting questions\n",
      "-- Decompressing Posts.7z\n",
      "-- Selecting questions from Posts.xml\n",
      "-- Splitting questions into chunks\n",
      "-- 1 splits\n",
      "-- Parsing questions\n",
      "--- Worker 0 started\n",
      "- Extracting duplicate pairs\n",
      "-- Decompressing PostLinks.7z\n",
      "-- Parsing duplicate pairs\n",
      "- Extracting answers\n",
      "-- Decompressing Posts.7z\n",
      "-- Selecting answers from Posts.xml\n",
      "-- Splitting answers into chunks\n",
      "-- 1 splits\n",
      "-- Parsing answers\n",
      "--- Worker 0 started\n"
     ]
    }
   ],
   "source": [
    "def parse_questions_xml(questions_path):\n",
    "    \"\"\"Parses XML files containing question data\"\"\"\n",
    "    questions = []\n",
    "\n",
    "    for _, node in iterparse(questions_path, events=(\"end\",)):\n",
    "        if node.tag == \"row\":  # ignore starting and ending <post> tags\n",
    "            questions.append(\n",
    "                {\n",
    "                    \"id\": node.attrib.get(\"Id\"),\n",
    "                    \"title\": node.attrib.get(\"Title\"),\n",
    "                    \"body\": node.attrib.get(\"Body\"),\n",
    "                    \"tags\": node.attrib.get(\"Tags\"),\n",
    "                    \"accepted_answer\": node.attrib.get(\"AcceptedAnswerId\"),\n",
    "                    \"n_answers\": node.attrib.get(\"AnswerCount\"),\n",
    "                }\n",
    "            )\n",
    "        node.clear()\n",
    "\n",
    "    questions = pd.DataFrame(questions)\n",
    "    # select accepted answers to append to answer data later\n",
    "    accepted_answers = questions[[\"accepted_answer\"]].dropna()\n",
    "    questions = questions[[\"id\", \"n_answers\", \"title\", \"body\", \"tags\"]]\n",
    "\n",
    "    questions = questions.drop_duplicates(\"id\")\n",
    "    questions[\"n_answers\"] = questions[\"n_answers\"].apply(int)\n",
    "\n",
    "    # make the string of tags comma separated (\"<tag1><tag2>\" -> \"tag1,tag2\")\n",
    "    split_tag = lambda s: s.replace(\"><\", \",\").replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    questions.tags = questions.tags.apply(split_tag)\n",
    "\n",
    "    return questions, accepted_answers\n",
    "\n",
    "\n",
    "def question_parser(i, ds, questions_xml):\n",
    "    \"\"\"Function for one subprocess parsing XML question data\n",
    "    i -> number of the worker\n",
    "    \"\"\"\n",
    "    print(f\"--- Worker {i} started\")\n",
    "    questions, acc_ids = parse_questions_xml(questions_xml)\n",
    "\n",
    "    qids = questions[[\"id\"]]  # save question IDs to select dup pairs later\n",
    "\n",
    "    save(questions, paths.question_texts(ds, i))\n",
    "    # Remove the raw XML file to save space\n",
    "    questions_xml.unlink()\n",
    "\n",
    "    return qids, acc_ids\n",
    "\n",
    "\n",
    "def extract_questions_xml(ds):\n",
    "    \"\"\"Extract all questions from XML archives for a given dataset\"\"\"\n",
    "    print(\"- Extracting questions\")\n",
    "    post_type = post_types[\"questions\"]\n",
    "\n",
    "    select_post_type(ds, post_type)\n",
    "    splits = split_xml_file(ds, post_type)\n",
    "\n",
    "    print(\"-- Parsing questions\")\n",
    "\n",
    "    # multiprocess to increase speed. Reduce n_procs to save on memory\n",
    "    args = [(i, ds, s) for i, s in enumerate(splits)]\n",
    "    with Pool(n_procs) as p:\n",
    "        res = p.starmap(question_parser, args)\n",
    "\n",
    "    # suffix is unnecessary if there is only one split (less than 1M questions)\n",
    "    if len(splits) == 1:\n",
    "        paths.question_texts(ds, 0).rename(paths.question_texts(ds))\n",
    "\n",
    "    # save list of all question ids\n",
    "    qids = [r[0] for r in res]\n",
    "    qids = pd.concat(qids).reset_index(drop=True)\n",
    "    qids = qids.drop_duplicates()\n",
    "    save(qids, paths.all_question_ids(ds))\n",
    "\n",
    "    # save list of all accepted answers\n",
    "    acc_ids = [r[1] for r in res]\n",
    "    acc_ids = pd.concat(acc_ids).reset_index(drop=True)\n",
    "    acc_ids = acc_ids.drop_duplicates()\n",
    "    acc_ids = acc_ids.rename(columns={\"accepted_answer\": \"id\"})\n",
    "    acc_ids = acc_ids[[\"id\"]]\n",
    "    save(acc_ids, paths.accepted_answer_ids(ds))\n",
    "\n",
    "\n",
    "def parse_postlinks_xml(ds):\n",
    "    \"\"\"Parse and select all duplicate relations from PostLinks.xml for a given dataset\"\"\"\n",
    "    pairs = []\n",
    "    links_xml = paths.post_links_xml(ds)\n",
    "\n",
    "    for _, node in iterparse(links_xml, events=(\"end\",)):\n",
    "        # LinkTypeId for duplicate relation is 3\n",
    "        if node.attrib.get(\"LinkTypeId\") == \"3\":\n",
    "            pairs.append(\n",
    "                {\n",
    "                    \"dup_id\": node.attrib.get(\"PostId\"),\n",
    "                    \"main_id\": node.attrib.get(\"RelatedPostId\"),\n",
    "                }\n",
    "            )\n",
    "        node.clear()\n",
    "\n",
    "    pairs = pd.DataFrame(pairs)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def extract_dup_pairs_xml(ds):\n",
    "    \"\"\"Extract duplicate question pairs from XML archives for a given dataset\"\"\"\n",
    "    print(\"- Extracting duplicate pairs\")\n",
    "    extract_post_links(ds)\n",
    "\n",
    "    print(\"-- Parsing duplicate pairs\")\n",
    "    dup_pairs = parse_postlinks_xml(ds)\n",
    "\n",
    "    # Select only the pairs that have both questions in the set of question IDs\n",
    "    qids = read(paths.all_question_ids(ds)).id\n",
    "\n",
    "    dup_in_qs = dup_pairs.dup_id.isin(qids)\n",
    "    main_in_qs = dup_pairs.main_id.isin(qids)\n",
    "\n",
    "    dup_pairs = dup_pairs[dup_in_qs & main_in_qs]\n",
    "    dup_pairs = dup_pairs.drop_duplicates()\n",
    "\n",
    "    save(dup_pairs, paths.dup_pairs(ds))\n",
    "\n",
    "    # remove PostLinks.xml to save space\n",
    "    paths.post_links_xml(ds).unlink()\n",
    "\n",
    "\n",
    "def parse_answers_xml(ds, answers_path):\n",
    "    \"\"\"Parses XML files containing answer data\"\"\"\n",
    "    qids = read(paths.all_question_ids(ds)).id\n",
    "    acc_ids = read(paths.accepted_answer_ids(ds)).id\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    for _, node in iterparse(answers_path, events=(\"end\",)):\n",
    "        if node.tag == \"row\":\n",
    "            answers.append(\n",
    "                {\n",
    "                    \"id\": node.attrib.get(\"Id\"),\n",
    "                    \"question_id\": node.attrib.get(\"ParentId\"),\n",
    "                    \"score\": node.attrib.get(\"Score\"),\n",
    "                    \"body\": node.attrib.get(\"Body\"),\n",
    "                    \"post_date\": node.attrib.get(\"CreationDate\"),\n",
    "                }\n",
    "            )\n",
    "        node.clear()\n",
    "\n",
    "    answers = pd.DataFrame(answers)\n",
    "\n",
    "    # only select answers that have a question in the dataset\n",
    "    answers = answers[answers.question_id.isin(qids)]\n",
    "\n",
    "    answers[\"score\"] = answers.score.apply(int)\n",
    "\n",
    "    # mark answers as accepted\n",
    "    answers.loc[answers.id.isin(acc_ids), \"accepted\"] = True\n",
    "    answers[\"accepted\"] = answers[\"accepted\"].fillna(False)\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def answer_parser(i, ds, answers_xml):\n",
    "    \"\"\"Function for one subprocess parsing XML answer data\n",
    "    i -> number of the worker\n",
    "    \"\"\"\n",
    "    print(f\"--- Worker {i} started\")\n",
    "    answers = parse_answers_xml(ds, answers_xml)\n",
    "\n",
    "    save_path = paths.answer_texts(ds, i)\n",
    "\n",
    "    save(answers, save_path)\n",
    "    # Removes XML file to save space\n",
    "    answers_xml.unlink()\n",
    "\n",
    "\n",
    "def extract_answers_xml(ds):\n",
    "    \"\"\"Extract all questions from XML archives for a given dataset\"\"\"\n",
    "    print(\"- Extracting answers\")\n",
    "\n",
    "    post_type = post_types[\"answers\"]\n",
    "\n",
    "    select_post_type(ds, post_type)\n",
    "    splits = split_xml_file(ds, post_type)\n",
    "\n",
    "    print(\"-- Parsing answers\")\n",
    "\n",
    "    # multiprocess to increase speed. Reduce n_procs to save on memory\n",
    "    args = [(i, ds, s) for i, s in enumerate(splits)]\n",
    "    with Pool(n_procs) as p:\n",
    "        p.starmap(answer_parser, args)\n",
    "\n",
    "    # suffix is unnecessary if there is only one split (less than 1M questions)\n",
    "    if len(splits) == 1:\n",
    "        paths.answer_texts(ds, 0).rename(paths.answer_texts(ds))\n",
    "\n",
    "    # remove accepted answer IDs file as it won't be used anymore\n",
    "    paths.accepted_answer_ids(ds).unlink()\n",
    "\n",
    "\n",
    "def extract_xml(ds):\n",
    "    \"\"\"Extract questions, answers and dup pairs from an XML archive\"\"\"\n",
    "    extract_questions_xml(ds)\n",
    "    extract_dup_pairs_xml(ds)\n",
    "    extract_answers_xml(ds)\n",
    "\n",
    "\n",
    "def extract_xml_datasets(datasets):\n",
    "    unpack_gamedev_se()\n",
    "    for ds in datasets:\n",
    "        print(f\"Extracting data from {ds}\")\n",
    "        extract_xml(ds)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Extracting XML data\")\n",
    "    extract_xml_datasets([\"gamedev_se\", \"stackoverflow\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6730e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting Game Dev. SO questions\n",
      "-- Selecting questions\n",
      "-- Selecting answers                              \n",
      "-- Selecting dup pairs\n",
      "Sampling Stack Overflow\n",
      "- Selecting sample 0\n",
      "-- Selecting dup pairs\n",
      "-- Selecting questions\n",
      "-- Selecting answers                              \n",
      "- Selecting sample 1                              \n",
      "-- Selecting dup pairs\n",
      "-- Selecting questions\n",
      "-- Selecting answers                              \n",
      "- Selecting sample 2                              \n",
      "-- Selecting dup pairs\n",
      "-- Selecting questions\n",
      "-- Selecting answers                              \n",
      "- Selecting sample 3                              \n",
      "-- Selecting dup pairs\n",
      "-- Selecting questions\n",
      "-- Selecting answers                              \n",
      "- Selecting sample 4                              \n",
      "-- Selecting dup pairs\n",
      "-- Selecting questions\n",
      "-- Selecting answers                              \n",
      "                                                  \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import paths, save, read\n",
    "from utils.consts import gamedev_tags, so_sample_seeds\n",
    "\n",
    "\n",
    "def sample_stackoverflow(sample_num, seed):\n",
    "    \"\"\"Creates a sample from the StackOverflow dataset of similar size to the game dev. datasets\"\"\"\n",
    "\n",
    "    def select_dup_pairs():\n",
    "        \"\"\"Selects a number of dup pairs equal to the mean of dup pairs in game dev datasets\"\"\"\n",
    "        print(\"-- Selecting dup pairs\")\n",
    "        len_pairs_se = len(read(paths.dup_pairs(\"gamedev_se\")))\n",
    "        len_pairs_so = len(read(paths.dup_pairs(\"gamedev_so\")))\n",
    "        len_pairs = (len_pairs_se + len_pairs_so) // 2\n",
    "\n",
    "        pairs = read(paths.dup_pairs(\"stackoverflow\"))\n",
    "        pairs = pairs.drop_duplicates()\n",
    "        pairs = pairs.sample(n=len_pairs, random_state=seed).reset_index(drop=True)\n",
    "        save(pairs, paths.dup_pairs(f\"so_samples/sample_{sample_num}\"))\n",
    "\n",
    "    def select_questions():\n",
    "        \"\"\"Selects a number of questions equal to the mean questions in game dev datasets\"\"\"\n",
    "        print(\"-- Selecting questions\")\n",
    "        pairs = read(paths.dup_pairs(f\"so_samples/sample_{sample_num}\"))\n",
    "\n",
    "        len_se = len(read(paths.question_texts(\"gamedev_se\")))\n",
    "        len_so = len(read(paths.question_texts(\"gamedev_so\")))\n",
    "        sample_size = (len_se + len_so) // 2\n",
    "        remaining_questions = sample_size\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        n_splits = len(list(paths.corpus_dir(\"stackoverflow\").glob(\"question_texts*\")))\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            print(i, end=\"\\r\")\n",
    "            # samples to select in this split\n",
    "            n_samples = (sample_size // n_splits) + int(i < sample_size % n_splits)\n",
    "\n",
    "            # if there is only one split we don't have suffixes\n",
    "            if n_splits == 1:\n",
    "                i = None\n",
    "\n",
    "            df = read(paths.question_texts(\"stackoverflow\", i))\n",
    "\n",
    "            # questions in the pre-selected dup pairs\n",
    "            is_in_pairs = df.id.isin(pairs.main_id) | df.id.isin(pairs.dup_id)\n",
    "\n",
    "            df_dups = df[is_in_pairs]\n",
    "            # questions not in pairs\n",
    "            df = df[~is_in_pairs]\n",
    "\n",
    "            # random questions to sample from this split\n",
    "            to_sample = n_samples - len(df_dups)\n",
    "            sample = df.sample(n=to_sample, random_state=seed)\n",
    "\n",
    "            df = pd.concat([sample, df_dups]).reset_index(drop=True)\n",
    "            dfs.append(df)\n",
    "            print(\" \" * 50, end=\"\\r\")\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "        df = df.drop_duplicates(\"id\")\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        save(df, paths.question_texts(f\"so_samples/sample_{sample_num}\"))\n",
    "\n",
    "    def select_answers():\n",
    "        \"\"\"Selects only the answers that have questions in the sampled dataset\"\"\"\n",
    "        print(\"-- Selecting answers\")\n",
    "        qids = read(paths.question_texts(f\"so_samples/sample_{sample_num}\")).id\n",
    "\n",
    "        df = []\n",
    "        n_splits = len(list(paths.corpus_dir(\"stackoverflow\").glob(\"answer_texts*\")))\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            print(i, end=\"\\r\")\n",
    "\n",
    "            # if there is only one split we don't have suffixes\n",
    "            if n_splits == 1:\n",
    "                i = None\n",
    "\n",
    "            split = read(paths.answer_texts(\"stackoverflow\", i))\n",
    "\n",
    "            df_split = split[split.question_id.isin(qids)]\n",
    "            df.append(df_split)\n",
    "            print(\" \" * 50, end=\"\\r\")\n",
    "\n",
    "        df = pd.concat(df)\n",
    "        df = df.reset_index(drop=True)\n",
    "        save(df, paths.answer_texts(f\"so_samples/sample_{sample_num}\"))\n",
    "\n",
    "    select_dup_pairs()\n",
    "    select_questions()\n",
    "    select_answers()\n",
    "\n",
    "\n",
    "def select_gamedev(tags):\n",
    "    \"\"\"Selects posts related to game dev based on the given tags\"\"\"\n",
    "\n",
    "    def select_questions():\n",
    "        \"\"\"Selects game dev questions from each split if they contain one of the tags\"\"\"\n",
    "        print(\"-- Selecting questions\")\n",
    "        df = []\n",
    "        n_splits = len(list(paths.corpus_dir(\"stackoverflow\").glob(\"question_texts*\")))\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            # if there is only one split we don't have suffixes\n",
    "            if n_splits == 1:\n",
    "                i = None\n",
    "\n",
    "            split = read(paths.question_texts(\"stackoverflow\", i))\n",
    "\n",
    "            for t in tags:\n",
    "                print(i, t, end=\"\\r\")\n",
    "                # selects questions that have tag 't' in the list\n",
    "                tag_in_list = lambda ts: t.lower() in ts.lower().split(\",\")\n",
    "                df_tag = split[split.tags.apply(tag_in_list)]\n",
    "                df.append(df_tag)\n",
    "                print(\" \" * 50, end=\"\\r\")\n",
    "\n",
    "        df = pd.concat(df)\n",
    "        df = df.drop_duplicates(\"id\").reset_index(drop=True)\n",
    "        save(df, paths.question_texts(\"gamedev_so\"))\n",
    "\n",
    "    def select_answers():\n",
    "        \"\"\"Selects only the answers that have questions in the game dev dataset\"\"\"\n",
    "        print(\"-- Selecting answers\")\n",
    "        qids = read(paths.question_texts(\"gamedev_so\")).id\n",
    "\n",
    "        df = []\n",
    "        n_splits = len(list(paths.corpus_dir(\"stackoverflow\").glob(\"answer_texts*\")))\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            print(i, end=\"\\r\")\n",
    "\n",
    "            # if there is only one split we don't have suffixes\n",
    "            if n_splits == 1:\n",
    "                i = None\n",
    "\n",
    "            split = read(paths.answer_texts(\"stackoverflow\", i))\n",
    "\n",
    "            df_split = split[split.question_id.isin(qids)]\n",
    "            df.append(df_split)\n",
    "\n",
    "        df = pd.concat(df)\n",
    "        df = df.reset_index(drop=True)\n",
    "        save(df, paths.answer_texts(\"gamedev_so\"))\n",
    "\n",
    "    def select_dup_pairs():\n",
    "        \"\"\"Selects only the dup pairs that have both questions in the game dev dataset\"\"\"\n",
    "        print(\"-- Selecting dup pairs\")\n",
    "        qids = read(paths.question_texts(\"gamedev_so\")).id\n",
    "\n",
    "        pairs = read(paths.dup_pairs(\"stackoverflow\"))\n",
    "        pairs = pairs[pairs.main_id.isin(qids) & pairs.dup_id.isin(qids)]\n",
    "        pairs = pairs.drop_duplicates()\n",
    "        pairs = pairs.reset_index(drop=True)\n",
    "\n",
    "        save(pairs, paths.dup_pairs(\"gamedev_so\"))\n",
    "\n",
    "    select_questions()\n",
    "    select_answers()\n",
    "    select_dup_pairs()\n",
    "\n",
    "\n",
    "def select_so_samples(seeds):\n",
    "    \"\"\"Select one sample for each given seed\"\"\"\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"- Selecting sample {i}\")\n",
    "        sample_stackoverflow(i, seed)\n",
    "\n",
    "\n",
    "def main(tags_gamedev, seeds):\n",
    "    print(\"Selecting Game Dev. SO questions\")\n",
    "    select_gamedev(tags_gamedev)\n",
    "    print(\"Sampling Stack Overflow\")\n",
    "    select_so_samples(seeds)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(gamedev_tags, so_sample_seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c500edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing texts.\n",
      "- Preprocessing texts for gamedev_se\n",
      "- Preprocessing texts for gamedev_so\n",
      "- Preprocessing texts for so_samples/sample_0\n",
      "- Preprocessing texts for so_samples/sample_1\n",
      "- Preprocessing texts for so_samples/sample_2\n",
      "- Preprocessing texts for so_samples/sample_3\n",
      "- Preprocessing texts for so_samples/sample_4\n",
      "Finished preprocessing texts.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "from utils import paths, read, save\n",
    "from utils.consts import datasets, n_procs\n",
    "\n",
    "\n",
    "def process_html(t):\n",
    "    \"\"\"Processes HTML text to replace or remove tags\"\"\"\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"\\n\", \" \", t)\n",
    "    t = re.sub(r\"<code>.*?</code>\", \" codesnippet \", t)  # replace code\n",
    "    t = re.sub(r\"<a.*?https?:\\/\\/.*?[\\b\\s]?>\", \" url \", t)  # replace urls\n",
    "    t = re.sub(r\"https?:\\/\\/.*?(?:[\\b\\s]|$)\", \" url \", t)  # replace urls\n",
    "    t = re.sub(r\"<img.*?>\", \" img \", t)  # replace images\n",
    "    t = BeautifulSoup(t, features=\"lxml\").get_text()  # remove html tags\n",
    "    return t\n",
    "\n",
    "\n",
    "def preprocess_texts(df):\n",
    "    \"\"\"Applies the process_html function to different question parts\n",
    "    and merges them\n",
    "    \"\"\"\n",
    "    df[\"body\"] = df[\"body\"].apply(process_html)\n",
    "    df[\"title\"] = df[\"title\"].apply(process_html)\n",
    "    df[\"answer\"] = df[\"answer\"].apply(process_html)\n",
    "    df[\"title_body\"] = df.title + \" \" + df.body\n",
    "    df[\"title_body_tags\"] = df.title_body + \" \" + df.tags\n",
    "    df[\"title_body_tags_answer\"] = df.title_body_tags + \" \" + df.answer\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenize_texts(df):\n",
    "    \"\"\"Tokenizes each question part using preprocess_string and merges them\"\"\"\n",
    "    df[\"body\"] = df[\"body\"].apply(preprocess_string)\n",
    "    df[\"title\"] = df[\"title\"].apply(preprocess_string)\n",
    "    df[\"tags\"] = df[\"tags\"].apply(preprocess_string)\n",
    "    df[\"answer\"] = df[\"answer\"].apply(preprocess_string)\n",
    "    df[\"title_body\"] = df.title + df.body\n",
    "    df[\"title_body_tags\"] = df.title_body + df.tags\n",
    "    df[\"title_body_tags_answer\"] = df.title_body_tags + df.answer\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_pool(df, f):\n",
    "    \"\"\"Splits the dataframe into chunks and\n",
    "    maps the function using multiprocessing\n",
    "    \"\"\"\n",
    "    dfs = np.array_split(df, n_procs)\n",
    "\n",
    "    with Pool(n_procs) as p:\n",
    "        dfs = p.map(f, dfs)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_questions(ds):\n",
    "    \"\"\"Reads the set of questions for a dataset and normalizes it\"\"\"\n",
    "    df = read(paths.question_texts(ds))\n",
    "\n",
    "    assert not df.isna().any().any(), \"NaN columns in data!\"\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates(\"id\")\n",
    "\n",
    "    # get the ordered index of each question in the dataframe\n",
    "    df = df.reset_index(drop=True).reset_index()\n",
    "    df = df.rename(columns={\"index\": \"corpus_index\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_best_answers(answers):\n",
    "    \"\"\"Selects the best answer for a given question\n",
    "    based on the heuristic:\n",
    "    accepted answer > highest score > posted first\n",
    "    \"\"\"\n",
    "    answers[\"max_score\"] = answers.groupby(\"question_id\").score.apply(\n",
    "        lambda s: s == max(s)\n",
    "    )\n",
    "    answers[\"posted_first\"] = answers.groupby(\"question_id\").post_date.apply(\n",
    "        lambda s: s == min(s)\n",
    "    )\n",
    "\n",
    "    # subset of answers that *can* be the best one\n",
    "    best_answers = answers[answers.accepted | answers.max_score | answers.posted_first]\n",
    "\n",
    "    # sort the answers according to the criteria\n",
    "    sort_cols = [\"accepted\", \"max_score\", \"posted_first\"]\n",
    "    best_answers = best_answers.sort_values(sort_cols, ascending=False)\n",
    "\n",
    "    # the first sorted answer is the \"best\" one\n",
    "    best_answers = best_answers.groupby(\"question_id\").first().reset_index()\n",
    "\n",
    "    assert (\n",
    "        best_answers.groupby(\"question_id\").apply(len) == 1\n",
    "    ).all(), \"Some questions have more than one answer!\"\n",
    "    assert answers.question_id.isin(best_answers.question_id).all(), \"Missing answers!\"\n",
    "    assert (\n",
    "        answers[answers.accepted].question_id.isin(best_answers.question_id).all()\n",
    "    ), \"Missing accepted answers!\"\n",
    "\n",
    "    return best_answers\n",
    "\n",
    "\n",
    "def get_answers(ds):\n",
    "    \"\"\"Reads the set of answers for a dataset, selects the best one\n",
    "    and normalizes them\n",
    "    \"\"\"\n",
    "    df = read(paths.answer_texts(ds))\n",
    "    df = df.drop_duplicates()\n",
    "    df = select_best_answers(df)\n",
    "    df = df[[\"question_id\", \"body\"]]\n",
    "    df.columns = [\"id\", \"answer\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def questions_with_answers(ds):\n",
    "    \"\"\"Reads questions and answers for a given dataset and merges them\"\"\"\n",
    "    questions = get_questions(ds)\n",
    "    answers = get_answers(ds)\n",
    "\n",
    "    questions = questions.merge(answers, on=\"id\", how=\"left\")\n",
    "    questions[\"answer\"] = questions.answer.fillna(\"\")\n",
    "\n",
    "    assert (questions.groupby(\"id\").apply(len) == 1).all(), \"Duplicate questions!\"\n",
    "\n",
    "    questions = questions.set_index(\"id\")\n",
    "\n",
    "    return questions\n",
    "\n",
    "\n",
    "def preprocess_questions(ds):\n",
    "    \"\"\"Reads questions with answers for a dataset and preprocesses their texts\"\"\"\n",
    "    print(f\"- Preprocessing texts for {ds}\")\n",
    "    df = questions_with_answers(ds)\n",
    "\n",
    "    df = map_pool(df, preprocess_texts)\n",
    "    save(df, paths.corpus(ds, tokenized=False))\n",
    "\n",
    "    df = map_pool(df, tokenize_texts)\n",
    "    save(df, paths.corpus(ds, tokenized=True))\n",
    "\n",
    "\n",
    "def main(datasets):\n",
    "    print(f\"Preprocessing texts.\")\n",
    "    for ds in datasets:\n",
    "        preprocess_questions(ds)\n",
    "    print(f\"Finished preprocessing texts.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde1b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting IDs.\n",
      "Extracting IDs for gamedev_se.\n",
      "- 1222 dup pairs\n",
      "-- 1170 dups\n",
      "-- 856 main questions\n",
      "- 54684 questions\n",
      "- 47101 answered questions\n",
      "- 936 dups in the train set\n",
      "- 234 dups in the test set\n",
      "Extracting IDs for gamedev_so.\n",
      "- 7 dup pairs\n",
      "-- 7 dups\n",
      "-- 6 main questions\n",
      "- 906 questions\n",
      "- 744 answered questions\n",
      "- 6 dups in the train set\n",
      "- 1 dups in the test set\n",
      "Extracting IDs for so_samples/sample_0.\n",
      "- 614 dup pairs\n",
      "-- 602 dups\n",
      "-- 488 main questions\n",
      "- 27795 questions\n",
      "- 23851 answered questions\n",
      "- 482 dups in the train set\n",
      "- 120 dups in the test set\n",
      "Extracting IDs for so_samples/sample_1.\n",
      "- 614 dup pairs\n",
      "-- 600 dups\n",
      "-- 481 main questions\n",
      "- 27795 questions\n",
      "- 23871 answered questions\n",
      "- 480 dups in the train set\n",
      "- 120 dups in the test set\n",
      "Extracting IDs for so_samples/sample_2.\n",
      "- 614 dup pairs\n",
      "-- 602 dups\n",
      "-- 468 main questions\n",
      "- 27795 questions\n",
      "- 23884 answered questions\n",
      "- 482 dups in the train set\n",
      "- 120 dups in the test set\n",
      "Extracting IDs for so_samples/sample_3.\n",
      "- 614 dup pairs\n",
      "-- 599 dups\n",
      "-- 492 main questions\n",
      "- 27795 questions\n",
      "- 23937 answered questions\n",
      "- 479 dups in the train set\n",
      "- 120 dups in the test set\n",
      "Extracting IDs for so_samples/sample_4.\n",
      "- 614 dup pairs\n",
      "-- 600 dups\n",
      "-- 497 main questions\n",
      "- 27795 questions\n",
      "- 23927 answered questions\n",
      "- 480 dups in the train set\n",
      "- 120 dups in the test set\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import paths, read, save\n",
    "from utils.consts import datasets, split_percentage, noise_percentage\n",
    "\n",
    "\n",
    "def extract_dup_pair_ids(ds):\n",
    "    \"\"\"Extracts the IDs of dup pairs in the dataset\"\"\"\n",
    "    pairs = read(paths.dup_pairs(ds))\n",
    "    corpus = read(paths.corpus(ds)).reset_index()\n",
    "\n",
    "    print(f\"- {len(pairs)} dup pairs\")\n",
    "\n",
    "    dups = corpus[corpus.id.isin(pairs.dup_id)]\n",
    "    dups = dups[[\"id\", \"corpus_index\"]]\n",
    "    dups = dups.reset_index(drop=True)\n",
    "\n",
    "    print(f\"-- {len(dups)} dups\")\n",
    "\n",
    "    save(dups, paths.duplicate_question_ids(ds))\n",
    "\n",
    "    main_qs = corpus[corpus.id.isin(pairs.main_id)]\n",
    "    main_qs = main_qs[[\"id\", \"corpus_index\"]]\n",
    "    main_qs = main_qs.reset_index(drop=True)\n",
    "\n",
    "    print(f\"-- {len(main_qs)} main questions\")\n",
    "\n",
    "    save(main_qs, paths.main_question_ids(ds))\n",
    "\n",
    "\n",
    "def extract_question_ids(ds):\n",
    "    \"\"\"Extracts question IDs for all questions in the dataset\"\"\"\n",
    "    df = read(paths.corpus(ds))\n",
    "    df = df.reset_index()\n",
    "\n",
    "    print(f\"- {len(df)} questions\")\n",
    "\n",
    "    answered_ids = df[df.n_answers > 0][[\"id\", \"corpus_index\"]]\n",
    "    answered_ids = answered_ids.reset_index(drop=True)\n",
    "    save(answered_ids, paths.answered_question_ids(ds))\n",
    "\n",
    "    print(f\"- {len(answered_ids)} answered questions\")\n",
    "\n",
    "    df = df[[\"id\", \"corpus_index\"]]\n",
    "    save(df, paths.all_question_ids(ds))\n",
    "\n",
    "    main_ids = read(paths.main_question_ids(ds))\n",
    "\n",
    "    # Questions that will be compared = answered + main questions\n",
    "    comp_ids = pd.concat([answered_ids, main_ids])\n",
    "    comp_ids = comp_ids.reset_index(drop=True)\n",
    "    comp_ids = comp_ids.drop_duplicates(\"id\")\n",
    "\n",
    "    save(comp_ids, paths.comparison_question_ids(ds))\n",
    "\n",
    "\n",
    "def sample_noise_questions(ds, perc):\n",
    "    \"\"\"Samples a percentage of questions to serve as noise in supervised learning\"\"\"\n",
    "    df = read(paths.corpus(ds))\n",
    "    dups = read(paths.duplicate_question_ids(ds))\n",
    "    mains = read(paths.main_question_ids(ds))\n",
    "\n",
    "    df = df.reset_index()[[\"id\", \"corpus_index\"]]\n",
    "    # exclude true duplicates and their pairs\n",
    "    df = df[~df.id.isin(dups.id) & ~df.id.isin(mains.id)]\n",
    "\n",
    "    # sample a percentage of the number of duplicates\n",
    "    n_dups = len(dups)\n",
    "    noise_dups = round(n_dups * perc)\n",
    "    noise = df.sample(noise_dups, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    save(noise, paths.noise_question_ids(ds))\n",
    "\n",
    "\n",
    "def split_train_test_dups(ds, perc):\n",
    "    \"\"\"Randomly splits the duplicates that will be used for train and test sets\"\"\"\n",
    "    dups = read(paths.duplicate_question_ids(ds))\n",
    "\n",
    "    test_dups = dups.sample(frac=perc, random_state=42)\n",
    "    test_dups = test_dups.reset_index(drop=True)\n",
    "\n",
    "    save(test_dups, paths.test_dup_ids(ds))\n",
    "\n",
    "    # remaining dups used for training\n",
    "    train_dups = dups[~dups.id.isin(test_dups.id)]\n",
    "    train_dups = train_dups.reset_index(drop=True)\n",
    "\n",
    "    save(train_dups, paths.train_dup_ids(ds))\n",
    "\n",
    "    print(f\"- {len(train_dups)} dups in the train set\")\n",
    "    print(f\"- {len(test_dups)} dups in the test set\")\n",
    "\n",
    "\n",
    "def extract_all_ids(ds, noise_p, split_p):\n",
    "    \"\"\"Extract all ids for a given dataset using the functions above\"\"\"\n",
    "    print(f\"Extracting IDs for {ds}.\")\n",
    "    extract_dup_pair_ids(ds)\n",
    "    extract_question_ids(ds)\n",
    "    sample_noise_questions(ds, noise_p)\n",
    "    split_train_test_dups(ds, split_p)\n",
    "\n",
    "\n",
    "def main(datasets, noise_p, split_p):\n",
    "    print(\"Extracting IDs.\")\n",
    "    for ds in datasets:\n",
    "        extract_all_ids(ds, noise_p, split_p)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(datasets, noise_percentage, split_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba029fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature models\n",
      "Training feature models for gamedev_se.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n",
      "Training feature models for gamedev_so.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n",
      "Training feature models for so_samples/sample_0.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n",
      "Training feature models for so_samples/sample_1.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n",
      "Training feature models for so_samples/sample_2.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n",
      "Training feature models for so_samples/sample_3.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n",
      "Training feature models for so_samples/sample_4.\n",
      "- Training TF-IDF models\n",
      "- Training BM25 models\n",
      "- Training LDA models\n",
      "- Training Doc2Vec models\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from scipy.sparse import save_npz, csr_matrix\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from utils import paths, read, save, make_dir\n",
    "from utils.consts import datasets, text_columns, features\n",
    "from utils.models import (\n",
    "    doc2vec_model,\n",
    "    bm25_model,\n",
    "    lda_model,\n",
    "    bertoverflow_model,\n",
    "    mpnet_model,\n",
    ")\n",
    "\n",
    "\n",
    "def train_bm25(ds, cols):\n",
    "    \"\"\"Trains and saves a BM25 model for each text column in the dataset\"\"\"\n",
    "    print(\"- Training BM25 models\")\n",
    "    feature_name = \"bm25\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds))\n",
    "\n",
    "    make_dir(paths.feature_model_dir(ds, feature_name))\n",
    "\n",
    "    for c in cols:\n",
    "        model_save_path = paths.feature_model(ds, feature_name, c)\n",
    "        bm25 = bm25_model(corpus[c])\n",
    "        bm25.save(model_save_path)\n",
    "\n",
    "\n",
    "def train_doc2vec(ds, cols):\n",
    "    \"\"\"Trains and saves a Doc2Vec model for each text column in the dataset\n",
    "    Also saves document embeddings learned by the models.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_doc2vec_from_file(c):\n",
    "        \"\"\"Trains a Doc2Vec model from a file containing the corpus (increased performance)\"\"\"\n",
    "        training_corpus = paths.feature_model_dir(ds, feature_name) / (c + \".txt\")\n",
    "\n",
    "        # create a space-separated text file\n",
    "        with open(training_corpus, \"w\") as f:\n",
    "            for l in corpus[c].apply(lambda s: \" \".join(s)):\n",
    "                f.write(l + \"\\n\")\n",
    "\n",
    "        model = doc2vec_model(str(training_corpus))\n",
    "        training_corpus.unlink()\n",
    "        return model\n",
    "\n",
    "    print(\"- Training Doc2Vec models\")\n",
    "    feature_name = \"doc2vec\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds))\n",
    "\n",
    "    make_dir(paths.feature_model_dir(ds, feature_name))\n",
    "    make_dir(paths.embedding_dir(ds, feature_name))\n",
    "\n",
    "    for c in cols:\n",
    "        model = train_doc2vec_from_file(c)\n",
    "        emb = model.dv.vectors\n",
    "\n",
    "        model_save_path = paths.feature_model(ds, feature_name, c)\n",
    "        model.save(str(model_save_path))\n",
    "\n",
    "        emb_save_path = paths.embedding(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, csr_matrix(emb))\n",
    "\n",
    "\n",
    "def train_tfidf(ds, cols):\n",
    "    \"\"\"Trains and saves a TF-IDF model for each text column in the dataset\n",
    "    Also saves document embeddings learned by the models.\n",
    "    \"\"\"\n",
    "    print(\"- Training TF-IDF models\")\n",
    "    feature_name = \"tfidf\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds))\n",
    "\n",
    "    make_dir(paths.feature_model_dir(ds, feature_name))\n",
    "    make_dir(paths.embedding_dir(ds, feature_name))\n",
    "\n",
    "    for c in cols:\n",
    "        # tf-idf takes space separated strings\n",
    "        corpus[c] = corpus[c].apply(lambda l: \" \".join(l))\n",
    "        tfidf = TfidfVectorizer().fit(corpus[c])\n",
    "        emb = tfidf.transform(corpus[c])\n",
    "\n",
    "        model_save_path = paths.feature_model(ds, feature_name, c)\n",
    "        joblib.dump(tfidf, model_save_path)\n",
    "\n",
    "        emb_save_path = paths.embedding(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, emb)\n",
    "\n",
    "\n",
    "def train_lda(ds, cols):\n",
    "    \"\"\"Trains and saves an LDA model for each text column in the dataset\n",
    "    Also saves document embeddings learned by the models.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_from_bow(c):\n",
    "        \"\"\"Trains an LDA model from a BoW + Vocab\"\"\"\n",
    "        vocab = Dictionary(corpus[c])\n",
    "        corpus[c] = corpus[c].apply(vocab.doc2bow)\n",
    "        lda = lda_model(corpus[c], vocab)\n",
    "        return lda\n",
    "\n",
    "    print(\"- Training LDA models\")\n",
    "\n",
    "    feature_name = \"topic\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds))\n",
    "\n",
    "    make_dir(paths.feature_model_dir(ds, feature_name))\n",
    "    make_dir(paths.embedding_dir(ds, feature_name))\n",
    "\n",
    "    for c in cols:\n",
    "        lda = train_from_bow(c)\n",
    "        emb = corpus[c].apply(lambda t: sparse2full(lda[t], 100))\n",
    "\n",
    "        model_save_path = paths.feature_model(ds, feature_name, c)\n",
    "        lda.save(str(model_save_path))\n",
    "\n",
    "        emb_save_path = paths.embedding(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, csr_matrix(list(emb)))\n",
    "\n",
    "\n",
    "def get_bertoverflow_embeddings(ds, cols, use_gpu=True):\n",
    "    \"\"\"Computes BERTOverflow embeddings and saves them\"\"\"\n",
    "    print(\"- Computing BERTOverflow embeddings\")\n",
    "\n",
    "    feature_name = \"bertoverflow\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds, tokenized=False))\n",
    "\n",
    "    model = bertoverflow_model()\n",
    "\n",
    "    make_dir(paths.embedding_dir(ds, feature_name))\n",
    "\n",
    "    device = None\n",
    "    if not use_gpu:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    for c in cols:\n",
    "        print(f\"-- Computing {c} embeddings with BERTOverflow for {ds}.\")\n",
    "        emb = model.encode(corpus[c], device=device, show_progress_bar=True)\n",
    "\n",
    "        emb_save_path = paths.embedding(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, csr_matrix(emb))\n",
    "\n",
    "\n",
    "def get_mpnet_embeddings(ds, cols, use_gpu=True):\n",
    "    \"\"\"Computes MPNet embeddings and saves them\"\"\"\n",
    "    print(\"- Computing MPNet embeddings\")\n",
    "    feature_name = \"mpnet\"\n",
    "\n",
    "    corpus = read(paths.corpus(ds, tokenized=False))\n",
    "\n",
    "    model = mpnet_model()\n",
    "\n",
    "    make_dir(paths.embedding_dir(ds, feature_name))\n",
    "\n",
    "    device = None\n",
    "    if not use_gpu:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    for c in cols:\n",
    "        print(f\"-- Computing {c} embeddings with MPNet for {ds}.\")\n",
    "        emb = model.encode(corpus[c], device=device, show_progress_bar=True)\n",
    "\n",
    "        emb_save_path = paths.embedding(ds, feature_name, c)\n",
    "        save_npz(emb_save_path, csr_matrix(emb))\n",
    "\n",
    "\n",
    "def train_all_models(ds, feats, cols):\n",
    "    \"\"\"Trains all feature models for the given dataset\"\"\"\n",
    "    print(f\"Training feature models for {ds}.\")\n",
    "    if \"tfidf\" in feats:\n",
    "        train_tfidf(ds, cols)\n",
    "    if \"bm25\" in feats:\n",
    "        train_bm25(ds, cols)\n",
    "    if \"topic\" in feats:\n",
    "        train_lda(ds, cols)\n",
    "    if \"doc2vec\" in feats:\n",
    "        train_doc2vec(ds, cols)\n",
    "    if \"bertoverflow\" in feats:\n",
    "        get_bertoverflow_embeddings(ds, cols)\n",
    "    if \"mpnet\" in feats:\n",
    "        get_mpnet_embeddings(ds, cols)\n",
    "\n",
    "\n",
    "def main(datasets, feats, cols):\n",
    "    print(f\"Training feature models\")\n",
    "    for ds in datasets:\n",
    "        train_all_models(ds, feats, cols)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(datasets, features, text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7672bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking pair ranks for gamedev_se\n",
      "- Computing pair ranks for jaccard\n",
      "- Computing pair ranks for tfidf\n",
      "- Computing pair ranks for bm25\n",
      "- Computing pair ranks for topic\n",
      "- Computing pair ranks for doc2vec\n",
      "Ranking pair ranks for gamedev_so\n",
      "- Computing pair ranks for jaccard\n",
      "- Computing pair ranks for tfidf\n",
      "- Computing pair ranks for bm25\n",
      "- Computing pair ranks for topic\n",
      "- Computing pair ranks for doc2vec\n",
      "Ranking pair ranks for so_samples/sample_0\n",
      "- Computing pair ranks for jaccard\n",
      "- Computing pair ranks for tfidf\n",
      "- Computing pair ranks for bm25\n",
      "- Computing pair ranks for topic\n",
      "- Computing pair ranks for doc2vec\n",
      "Ranking pair ranks for so_samples/sample_1\n",
      "- Computing pair ranks for jaccard\n",
      "- Computing pair ranks for tfidf\n",
      "- Computing pair ranks for bm25\n",
      "- Computing pair ranks for topic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from utils import paths, read, save, QuestionComp\n",
    "from utils.consts import datasets, features, text_columns, n_procs\n",
    "\n",
    "\n",
    "def score_dup_pairs(dups, ds, f, c):\n",
    "    \"\"\"Scores a set of duplicates against all other answered/main questions\n",
    "    in the dataset for a given similarity (feature + question part)\n",
    "    \"\"\"\n",
    "\n",
    "    def merge_scores(df, qs, scores):\n",
    "        \"\"\"Adds similarity scores to the dataframe of questions\"\"\"\n",
    "        dup_id, dup_index = df.name\n",
    "        qs = qs.copy()\n",
    "        qs[\"score\"] = scores[dup_index]\n",
    "        qs = qs.drop(columns=\"corpus_index\")\n",
    "        return qs\n",
    "\n",
    "    comp_qs = read(paths.comparison_question_ids(ds))[[\"id\", \"corpus_index\"]]\n",
    "    qc = QuestionComp(ds, f, c)\n",
    "\n",
    "    scores = qc.compare(dups[\"corpus_index\"], comp_qs[\"corpus_index\"])\n",
    "\n",
    "    # replace corpus index with dup index to select correct set of scores\n",
    "    dups = dups.drop(columns=\"corpus_index\").reset_index(drop=True)\n",
    "    dups = dups.reset_index()\n",
    "    dups = dups.rename(columns={\"id\": \"dup_id\"})\n",
    "\n",
    "    merge_dup_scores = lambda df: merge_scores(df, comp_qs, scores)\n",
    "\n",
    "    scores = dups.groupby([\"dup_id\", \"index\"]).apply(merge_dup_scores)\n",
    "    scores = scores.reset_index()\n",
    "    scores = scores.drop(columns=[\"index\"])\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def calculate_scores(ds, f, c):\n",
    "    \"\"\"Scores all duplicates against all other answered/main questions\n",
    "    in the dataset for a given similarity (feature + question part)\n",
    "    Uses multiprocessing + the score_dup_pairs function\n",
    "    \"\"\"\n",
    "    dups = read(paths.duplicate_question_ids(ds))\n",
    "\n",
    "    dups = np.array_split(dups, n_procs)\n",
    "    tups = [(d, ds, f, c) for d in dups]\n",
    "\n",
    "    with Pool(n_procs) as p:\n",
    "        dups = p.starmap(score_dup_pairs, tups)\n",
    "\n",
    "    dups = pd.concat(dups)\n",
    "\n",
    "    return dups\n",
    "\n",
    "\n",
    "def rank_dup_pairs(ds, f, c):\n",
    "    \"\"\"Ranks dup pairs against all other question pairs\n",
    "    in the dataset based on the similarity scores for a\n",
    "    feature + column\n",
    "    \"\"\"\n",
    "    scores = calculate_scores(ds, f, c)\n",
    "\n",
    "    scores = scores.rename(columns={\"id\": \"main_id\"})\n",
    "\n",
    "    # removes pairs of the same question\n",
    "    scores = scores[scores.main_id != scores.dup_id]\n",
    "\n",
    "    pairs = read(paths.dup_pairs(ds))\n",
    "    pairs[\"is_dup\"] = True\n",
    "\n",
    "    scores = scores.merge(pairs, on=[\"dup_id\", \"main_id\"], how=\"left\")\n",
    "    scores[\"is_dup\"] = scores.is_dup.fillna(False)\n",
    "\n",
    "    # get rank\n",
    "    scores[\"rank\"] = scores.groupby(\"dup_id\").score.rank(ascending=False)\n",
    "\n",
    "    scores = scores[scores.is_dup]\n",
    "    scores = scores.reset_index(drop=True)\n",
    "    scores = scores.drop(columns=[\"is_dup\", \"level_2\"])\n",
    "\n",
    "    save(scores, paths.pair_ranks(ds, f, c))\n",
    "\n",
    "\n",
    "def calculate_recall_rates(ds, feats, cols):\n",
    "    \"\"\"Calculate recall-rates@k based on the dup pairs ranks for a given dataset\"\"\"\n",
    "\n",
    "    def recall_rate(df, k):\n",
    "        \"\"\"Calculates the recall-rate@k for a dataset using the rank column\"\"\"\n",
    "        # rank <= k -> dup pair in top k results\n",
    "        has_dup_in_k = lambda r: (r <= k).any()\n",
    "        return df.groupby(\"dup_id\")[\"rank\"].apply(has_dup_in_k).mean()\n",
    "\n",
    "    recall_rates = []\n",
    "\n",
    "    for f in feats:\n",
    "        for c in cols:\n",
    "            scores = read(paths.pair_ranks(ds, f, c))\n",
    "\n",
    "            rates = {\n",
    "                \"feature\": f,\n",
    "                \"col\": c,\n",
    "            }\n",
    "\n",
    "            for i in [5, 10, 20]:\n",
    "                rates[f\"recall-rate@{i}\"] = recall_rate(scores, i)\n",
    "\n",
    "            recall_rates.append(rates)\n",
    "\n",
    "    recall_rates = pd.DataFrame(recall_rates)\n",
    "    save(recall_rates, paths.all_pair_ranks(ds))\n",
    "\n",
    "\n",
    "def dup_pair_ranks(ds, feats, columns):\n",
    "    \"\"\"Calculates the ranks of dup pais for all similarity scores in the dataset\"\"\"\n",
    "    print(f\"Ranking pair ranks for {ds}\")\n",
    "    res = []\n",
    "    for f in feats:\n",
    "        print(f\"- Computing pair ranks for {f}\")\n",
    "        for c in columns:\n",
    "            rank_dup_pairs(ds, f, c)\n",
    "\n",
    "\n",
    "def main(datasets, feats, columns):\n",
    "    for ds in datasets:\n",
    "        dup_pair_ranks(ds, feats, columns)\n",
    "        calculate_recall_rates(ds, feats, columns)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(datasets, features, text_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba8e214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting candidates for gamedev_se.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n",
      "Selecting candidates for gamedev_so.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n",
      "Selecting candidates for so_samples/sample_0.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n",
      "Selecting candidates for so_samples/sample_1.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n",
      "Selecting candidates for so_samples/sample_2.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n",
      "Selecting candidates for so_samples/sample_3.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n",
      "Selecting candidates for so_samples/sample_4.\n",
      "- Selecting train candidates\n",
      "-- Making train sets\n",
      "--- 1500, 0.01\n",
      "- Selecting test candidates\n",
      "- Merging all sets of candidates\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import paths, read, save, QuestionComp\n",
    "from utils.consts import datasets, n_candidates, undersampling_percentages\n",
    "\n",
    "\n",
    "def select_candidates(ds, dups, comp, n):\n",
    "    \"\"\"Selects n candidate questions for each dup from the set of provided questions\"\"\"\n",
    "\n",
    "    def dup_cands(df, ids, scores, n):\n",
    "        \"\"\"Selects the n candidates with the highest score for the dup in the df\"\"\"\n",
    "        dup_id, _, dup_index = df.name\n",
    "        ids = ids.copy()\n",
    "        ids[\"score\"] = scores[dup_index]\n",
    "        # remove pairs with same questions\n",
    "        ids = ids[ids.candidate_id != dup_id].copy()\n",
    "        ids = ids.sort_values(\"score\", ascending=False)[:n]\n",
    "        return ids\n",
    "\n",
    "    def get_scores(ds, dups, comp):\n",
    "        \"\"\"Gets the scores according to a pre-defined similarity to use\n",
    "        when selecting candidate questions\n",
    "        \"\"\"\n",
    "        qc = QuestionComp(ds, \"tfidf\", \"title_body_tags_answer\")\n",
    "        return qc.compare(dups[\"corpus_index\"], comp[\"corpus_index\"])\n",
    "\n",
    "    scores = get_scores(ds, dups, comp)\n",
    "\n",
    "    dups = dups.rename(columns={\"id\": \"dup_id\", \"corpus_index\": \"dup_corpus_index\"})\n",
    "\n",
    "    # get index of the duplicate in the df\n",
    "    dups = dups.reset_index()\n",
    "\n",
    "    comp = comp.rename(\n",
    "        columns={\"id\": \"candidate_id\", \"corpus_index\": \"candidate_corpus_index\"}\n",
    "    )\n",
    "\n",
    "    get_cands = lambda df: dup_cands(df, comp, scores, n)\n",
    "    candidates = dups.groupby([\"dup_id\", \"dup_corpus_index\", \"index\"]).apply(get_cands)\n",
    "\n",
    "    # fix index\n",
    "    candidates = candidates.reset_index(level=\"index\", drop=True)\n",
    "    candidates = candidates.reset_index()\n",
    "    candidates = candidates.drop(columns=\"level_2\")\n",
    "\n",
    "    same_ids = candidates[candidates.dup_id == candidates.candidate_id]\n",
    "    assert len(same_ids) == 0, \"Some dup IDs are the same as their related IDs!\"\n",
    "    assert (\n",
    "        not candidates[[\"dup_id\", \"candidate_id\"]].duplicated().any()\n",
    "    ), \"Duplicated candidates!\"\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def add_dup_labels(cands, pairs):\n",
    "    \"\"\"Adds labels indicating if candidate pairs are duplicates or not\"\"\"\n",
    "    pairs[\"is_dup\"] = True\n",
    "    cands = cands.merge(\n",
    "        pairs[[\"dup_id\", \"main_id\", \"is_dup\"]],\n",
    "        left_on=[\"dup_id\", \"candidate_id\"],\n",
    "        right_on=[\"dup_id\", \"main_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    cands = cands.drop(columns=\"main_id\")\n",
    "    cands[\"is_dup\"] = cands.is_dup.fillna(False)\n",
    "    return cands\n",
    "\n",
    "\n",
    "def add_noise_labels(cands, noise):\n",
    "    \"\"\"Adds labels indicating if candidate pairs are noise or not\"\"\"\n",
    "    noise[\"is_noise\"] = True\n",
    "    noise = noise.rename(columns={\"id\": \"dup_id\"})\n",
    "    cands = cands.merge(noise[[\"dup_id\", \"is_noise\"]], on=\"dup_id\", how=\"left\")\n",
    "    cands[\"is_noise\"] = cands.is_noise.fillna(False)\n",
    "    return cands\n",
    "\n",
    "\n",
    "def undersample(candidates, perc):\n",
    "    \"\"\"Undersamples the set of candidate pairs to achieve a desired percentage\n",
    "    of true duplicate pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def samples_per_dup():\n",
    "        \"\"\"Calculates how many false candidates pairs\n",
    "        we have to sample for each duplicate question\n",
    "        \"\"\"\n",
    "        neg_dups = negatives[\"dup_id\"].unique()\n",
    "\n",
    "        # the number of negatives for each positive sample\n",
    "        negs_per_pos = round(1 / perc - 1)\n",
    "\n",
    "        # total number of negatives and positives\n",
    "        n_negs = negs_per_pos * len(positives)\n",
    "        n_dups = len(neg_dups)\n",
    "\n",
    "        # lower bound of negatives per duplicate question\n",
    "        lower_n = n_negs // n_dups\n",
    "\n",
    "        # adds one additional false sample for each dup\n",
    "        # until there is no remainder left (n_negs % n_dups)\n",
    "        # guarantees that we will have exactly n_negs\n",
    "        # and that dups will have similar numbers of pairs\n",
    "        return {d: lower_n + int(i < n_negs % n_dups) for i, d in enumerate(neg_dups)}\n",
    "\n",
    "    def select_samples(dup_df):\n",
    "        \"\"\"Selects a number of samples for the duplicate question\n",
    "        in the df according to the values obtained by samples_per_dup\n",
    "        \"\"\"\n",
    "        return dup_df.sample(dup_samples[dup_df.name], random_state=42)\n",
    "\n",
    "    positives = candidates[candidates.is_dup]\n",
    "    negatives = candidates[~candidates.is_dup]\n",
    "    dup_samples = samples_per_dup()\n",
    "\n",
    "    negatives = negatives.groupby(\"dup_id\").apply(select_samples).reset_index(drop=True)\n",
    "\n",
    "    return pd.concat([positives, negatives]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def decrease_candidates(candidates, n):\n",
    "    \"\"\"Reduces the number of candidate pairs to n\n",
    "    This function is useful for selecting candidates only once for a large N\n",
    "    and then reducing the size if we need smaller Ns\n",
    "    \"\"\"\n",
    "    limit_cands = lambda df: df.sort_values(\"score\", ascending=False)[:n]\n",
    "    return candidates.groupby(\"dup_id\").apply(limit_cands).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def select_train_candidates_split(ds, dups, n, undersample_perc=None):\n",
    "    \"\"\"Selects n train candidates pairs for the given dups of the dataset\n",
    "    in chunks to save memory space\n",
    "    For datasets with large numbers of candidates it is easy to\n",
    "    fill up memory space\n",
    "    \"\"\"\n",
    "    # select candidates for each chunk\n",
    "    splits = 10\n",
    "    dups = np.array_split(dups, splits)\n",
    "\n",
    "    for i, ds in enumerate(dups):\n",
    "        print(i, end=\"\\r\")\n",
    "        ds = ds.reset_index(drop=True)\n",
    "        select_train_candidates_single(ds, ds, n, undersample_perc, i)\n",
    "\n",
    "    # merge candidates sampled above\n",
    "    candidates = []\n",
    "    for i in range(splits):\n",
    "        path = paths.train_candidate_pairs(ds, n, undersample_perc, i)\n",
    "        candidates.append(read(path))\n",
    "        path.unlink()\n",
    "\n",
    "    candidates = pd.concat(candidates).reset_index(drop=True)\n",
    "\n",
    "    save(candidates, paths.train_candidate_pairs(ds, n, undersample_perc))\n",
    "\n",
    "\n",
    "def select_train_candidates_single(ds, dups, n, undersample_perc=None, i=None):\n",
    "    \"\"\"Selects n train candidates pairs for the given dups of the dataset\n",
    "    the i parameter allows for saving chunks of candidates separately\n",
    "    \"\"\"\n",
    "    test = read(paths.test_dup_ids(ds))\n",
    "    comp = read(paths.comparison_question_ids(ds))\n",
    "    # remove test dups from the comparison questions\n",
    "    # to avoid leakage (comparing train dups with test dups)\n",
    "    comp = comp[~comp.id.isin(test.id)]\n",
    "\n",
    "    candidates = select_candidates(ds, dups, comp, n)\n",
    "\n",
    "    dup_pairs = read(paths.dup_pairs(ds))\n",
    "    candidates = add_dup_labels(candidates, dup_pairs)\n",
    "\n",
    "    noise = read(paths.noise_question_ids(ds))\n",
    "    candidates = add_noise_labels(candidates, noise)\n",
    "\n",
    "    if undersample_perc is not None:\n",
    "        candidates = undersample(candidates, undersample_perc)\n",
    "\n",
    "    save(candidates, paths.train_candidate_pairs(ds, n, undersample_perc, i))\n",
    "\n",
    "\n",
    "def select_train_candidates(ds, n, undersample_perc=None):\n",
    "    \"\"\"Selects n train candidates for all of the train dups and noise questions in the dataset\"\"\"\n",
    "    train = read(paths.train_dup_ids(ds))\n",
    "    # noise is added to avoid bias\n",
    "    noise = read(paths.noise_question_ids(ds))\n",
    "\n",
    "    dups = pd.concat([train, noise]).reset_index(drop=True)\n",
    "\n",
    "    print(\"- Selecting train candidates\")\n",
    "\n",
    "    # for datasets with >= 5000 dups, we split them to save memory space\n",
    "    if len(dups) < 5000:\n",
    "        select_train_candidates_single(ds, dups, n, undersample_perc)\n",
    "    else:\n",
    "        select_train_candidates_split(ds, dups, n, undersample_perc)\n",
    "\n",
    "\n",
    "def select_train_candidates_multi(ds, n_candidates, percentages):\n",
    "    \"\"\"Selects train candidate pairs for a given dataset\n",
    "    using different values of n and undersampling percentages\n",
    "    \"\"\"\n",
    "    # we only need to sample the max number of candidates\n",
    "    # then we can use the decrease_candidates function\n",
    "    # to limit the number of candidates\n",
    "    max_candidates = max(n_candidates)\n",
    "    select_train_candidates(ds, max_candidates)\n",
    "\n",
    "    candidates = read(paths.train_candidate_pairs(ds, max_candidates))\n",
    "\n",
    "    print(\"-- Making train sets\")\n",
    "    for c in n_candidates:\n",
    "        reduced = decrease_candidates(candidates, c)\n",
    "        for p in percentages:\n",
    "            print(f\"--- {c}, {p}\")\n",
    "            # undersample the dataset only if the percentage of\n",
    "            # duplicates is smaller than the undersampling percentage\n",
    "            if reduced.is_dup.mean() < p:\n",
    "                sampled = undersample(reduced, p)\n",
    "            else:\n",
    "                sampled = reduced\n",
    "\n",
    "            save(sampled, paths.train_candidate_pairs(ds, c, p))\n",
    "\n",
    "\n",
    "def select_test_candidates(ds, n):\n",
    "    \"\"\"Selects n test candidates for all of the test dups in the dataset\"\"\"\n",
    "    dups = read(paths.test_dup_ids(ds))\n",
    "    comp = read(paths.comparison_question_ids(ds))\n",
    "\n",
    "    dup_pairs = read(paths.dup_pairs(ds))\n",
    "\n",
    "    print(\"- Selecting test candidates\")\n",
    "\n",
    "    candidates = select_candidates(ds, dups, comp, n)\n",
    "    candidates = add_dup_labels(candidates, dup_pairs)\n",
    "    # no noise in the test\n",
    "    candidates[\"is_noise\"] = False\n",
    "\n",
    "    save(candidates, paths.test_candidate_pairs(ds))\n",
    "\n",
    "\n",
    "def select_test_candidates_multi(ds, n_candidates):\n",
    "    \"\"\"Selects test candidate pairs for a given dataset\n",
    "    using different values of n\n",
    "    \"\"\"\n",
    "    # we only need to sample the max number of candidates\n",
    "    # we can later limit the number of candidates used\n",
    "    # during the evaluation of the classifiers\n",
    "    max_candidates = max(n_candidates)\n",
    "    select_test_candidates(ds, max_candidates)\n",
    "\n",
    "\n",
    "def merge_candidates(ds, candidates, percentages):\n",
    "    \"\"\"Merges all of the sets of candidate pairs into a single dataset\n",
    "    This way we avoid having to compute and compare features multiple\n",
    "    times for candidate pairs that appear in many sets of candidates\n",
    "    \"\"\"\n",
    "    print(\"- Merging all sets of candidates\")\n",
    "    candidate_sets = [read(paths.test_candidate_pairs(ds))]\n",
    "    for p in percentages:\n",
    "        for c in candidates:\n",
    "            candidate_sets.append(read(paths.train_candidate_pairs(ds, c, p)))\n",
    "\n",
    "    candidate_sets = pd.concat(candidate_sets)\n",
    "    candidate_sets = candidate_sets.drop_duplicates()\n",
    "    candidate_sets = candidate_sets.reset_index(drop=True)\n",
    "\n",
    "    save(candidate_sets, paths.candidate_pairs(ds))\n",
    "\n",
    "\n",
    "def select_candidates_multi(ds, ns, ps):\n",
    "    \"\"\"Selects candidate sets for multiple ns and undersampling percentages\"\"\"\n",
    "    print(f\"Selecting candidates for {ds}.\")\n",
    "    select_train_candidates_multi(ds, ns, ps)\n",
    "    select_test_candidates_multi(ds, ns)\n",
    "    merge_candidates(ds, ns, ps)\n",
    "\n",
    "\n",
    "def select_candidates_single(ds, n, p):\n",
    "    \"\"\"Selects candidate sets for a single value of n and undersampling percentage\"\"\"\n",
    "    print(f\"Selecting candidates for {ds}.\")\n",
    "    select_train_candidates(ds, n, p)\n",
    "    select_test_candidates(ds, n)\n",
    "    merge_candidates(ds, [n], [p])\n",
    "\n",
    "\n",
    "def main(datasets, ns, ps):\n",
    "    # we select multiple candidate sets for the datasets\n",
    "    # for our analysis\n",
    "    for ds in datasets:\n",
    "        select_candidates_multi(ds, ns, ps)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(datasets, n_candidates, undersampling_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e261841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started computing the features for gamedev_se.\n",
      "- Computing jaccard features for gamedev_se.\n",
      "- Computing tfidf features for gamedev_se.\n",
      "- Computing bm25 features for gamedev_se.\n",
      "- Computing topic features for gamedev_se.\n",
      "- Computing doc2vec features for gamedev_se.\n",
      "Started computing the features for gamedev_so.\n",
      "- Computing jaccard features for gamedev_so.\n",
      "- Computing tfidf features for gamedev_so.\n",
      "- Computing bm25 features for gamedev_so.\n",
      "- Computing topic features for gamedev_so.\n",
      "- Computing doc2vec features for gamedev_so.\n",
      "Started computing the features for so_samples/sample_0.\n",
      "- Computing jaccard features for so_samples/sample_0.\n",
      "- Computing tfidf features for so_samples/sample_0.\n",
      "- Computing bm25 features for so_samples/sample_0.\n",
      "- Computing topic features for so_samples/sample_0.\n",
      "- Computing doc2vec features for so_samples/sample_0.\n",
      "Started computing the features for so_samples/sample_1.\n",
      "- Computing jaccard features for so_samples/sample_1.\n",
      "- Computing tfidf features for so_samples/sample_1.\n",
      "- Computing bm25 features for so_samples/sample_1.\n",
      "- Computing topic features for so_samples/sample_1.\n",
      "- Computing doc2vec features for so_samples/sample_1.\n",
      "Started computing the features for so_samples/sample_2.\n",
      "- Computing jaccard features for so_samples/sample_2.\n",
      "- Computing tfidf features for so_samples/sample_2.\n",
      "- Computing bm25 features for so_samples/sample_2.\n",
      "- Computing topic features for so_samples/sample_2.\n",
      "- Computing doc2vec features for so_samples/sample_2.\n",
      "Started computing the features for so_samples/sample_3.\n",
      "- Computing jaccard features for so_samples/sample_3.\n",
      "- Computing tfidf features for so_samples/sample_3.\n",
      "- Computing bm25 features for so_samples/sample_3.\n",
      "- Computing topic features for so_samples/sample_3.\n",
      "- Computing doc2vec features for so_samples/sample_3.\n",
      "Started computing the features for so_samples/sample_4.\n",
      "- Computing jaccard features for so_samples/sample_4.\n",
      "- Computing tfidf features for so_samples/sample_4.\n",
      "- Computing bm25 features for so_samples/sample_4.\n",
      "- Computing topic features for so_samples/sample_4.\n",
      "- Computing doc2vec features for so_samples/sample_4.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from utils import paths, read, save, QuestionComp\n",
    "from utils.consts import datasets, n_procs, features, text_columns\n",
    "\n",
    "\n",
    "def save_features(df, ds, feature_name, proc_num):\n",
    "    \"\"\"Saves only the relevant columns for a set of features\"\"\"\n",
    "\n",
    "    def cols_to_save(df):\n",
    "        return [\"dup_id\", \"candidate_id\"] + [c for c in df.columns if \"_sim\" in c]\n",
    "\n",
    "    save_path = paths.feature(ds, feature_name, proc_num)\n",
    "    cols = cols_to_save(df)\n",
    "    save(df[cols], save_path)\n",
    "\n",
    "\n",
    "def calc_feature(ds, feature, cols, proc_num=None):\n",
    "    \"\"\"Calculates the feature values (similarity scores) for candidates from a given dataset\n",
    "    proc_num serves to select a chunk of candidates as opposed to all of them\n",
    "    \"\"\"\n",
    "\n",
    "    def compare_pairs(df, qc, f, c):\n",
    "        dup_id, dup_index = df.name\n",
    "        cand_indexes = df[\"candidate_corpus_index\"]\n",
    "        df[f\"{c}_{f}_sim\"] = qc.compare(dup_index, cand_indexes)\n",
    "        return df\n",
    "\n",
    "    candidates = read(paths.candidate_pairs(ds, proc_num))\n",
    "\n",
    "    for c in cols:\n",
    "        qc = QuestionComp(ds, feature, c)\n",
    "        f = lambda df: compare_pairs(df, qc, feature, c)\n",
    "        candidates = candidates.groupby([\"dup_id\", \"dup_corpus_index\"]).apply(f)\n",
    "        candidates = candidates.reset_index(drop=True)\n",
    "\n",
    "    save_features(candidates, ds, feature, proc_num)\n",
    "\n",
    "\n",
    "def split_candidates(ds, n_procs):\n",
    "    \"\"\"Splits candidates into chunks to allow for multiprocessing\n",
    "    during feature calculation\n",
    "    \"\"\"\n",
    "    candidates = read(paths.candidate_pairs(ds))\n",
    "\n",
    "    dups = candidates[\"dup_id\"].unique()\n",
    "    np.random.shuffle(dups)\n",
    "    split_dups = np.array_split(dups, n_procs)\n",
    "\n",
    "    for i, c in enumerate(split_dups):\n",
    "        df = candidates[candidates.dup_id.isin(c)]\n",
    "        save(df, paths.candidate_pairs(ds, i))\n",
    "\n",
    "\n",
    "def merge_datasets(ds, n_procs, feats):\n",
    "    \"\"\"Merges the chunks of feature dataframes into a single dataframe\n",
    "    for each feature\n",
    "    \"\"\"\n",
    "    for f in feats:\n",
    "        # merge and save\n",
    "        fs = [read(paths.feature(ds, f, i)) for i in range(n_procs)]\n",
    "        df_feat = pd.concat(fs).reset_index(drop=True)\n",
    "        save(df_feat, paths.feature(ds, f))\n",
    "\n",
    "        # delete chunks\n",
    "        for i in range(n_procs):\n",
    "            paths.feature(ds, f, i).unlink()\n",
    "        # delete the empty dir\n",
    "        paths.feature(ds, f, 0).parent.rmdir()\n",
    "\n",
    "    # delete candidate chunks\n",
    "    for i in range(n_procs):\n",
    "        paths.candidate_pairs(ds, i).unlink()\n",
    "    # delete the empty dir\n",
    "    paths.candidate_pairs(ds, 0).parent.rmdir()\n",
    "\n",
    "\n",
    "def calc_features(ds, feats, cols):\n",
    "    \"\"\"Calculates all of the features for a given dataset using multiprocessing\"\"\"\n",
    "    print(f\"Started computing the features for {ds}.\")\n",
    "    split_candidates(ds, n_procs)\n",
    "\n",
    "    for f in feats:\n",
    "        print(f\"- Computing {f} features for {ds}.\")\n",
    "        params = [(ds, f, cols, i) for i in range(n_procs)]\n",
    "\n",
    "        with Pool(n_procs) as p:\n",
    "            p.starmap(calc_feature, params)\n",
    "\n",
    "    merge_datasets(ds, n_procs, feats)\n",
    "\n",
    "\n",
    "def main(datasets, feats, cols):\n",
    "    for ds in datasets:\n",
    "        calc_features(ds, feats, cols)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(datasets, features, text_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e167345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for gamedev_se\n",
      "- Making train set (1500, 0.01)\n",
      "Creating train and test sets for gamedev_so\n",
      "- Making train set (1500, 0.01)\n",
      "Creating train and test sets for so_samples/sample_0\n",
      "- Making train set (1500, 0.01)\n",
      "Creating train and test sets for so_samples/sample_1\n",
      "- Making train set (1500, 0.01)\n",
      "Creating train and test sets for so_samples/sample_2\n",
      "- Making train set (1500, 0.01)\n",
      "Creating train and test sets for so_samples/sample_3\n",
      "- Making train set (1500, 0.01)\n",
      "Creating train and test sets for so_samples/sample_4\n",
      "- Making train set (1500, 0.01)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import paths, read, save\n",
    "from utils.consts import features, datasets, n_candidates, undersampling_percentages\n",
    "\n",
    "\n",
    "def check_features(feats, cands):\n",
    "    \"\"\"Checks if the candidates dataframe is the same as the features\n",
    "    dataframe after merging (e.g., if there are no repeated entries)\n",
    "    \"\"\"\n",
    "    same_size = len(cands) == len(feats)\n",
    "    same_dups = set(cands.dup_id) == set(feats.dup_id)\n",
    "    same_rels = set(cands.candidate_id) == set(feats.candidate_id)\n",
    "\n",
    "    return same_size and same_dups and same_rels\n",
    "\n",
    "\n",
    "def make_test(ds, feats):\n",
    "    \"\"\"Creates a test dataset by joining features to the test candidate pairs\"\"\"\n",
    "    cands = read(paths.test_candidate_pairs(ds))\n",
    "    test = merge_features(ds, cands.copy(), feats)\n",
    "\n",
    "    assert check_features(\n",
    "        test, cands\n",
    "    ), \"The test dataset is different from its candidates!\"\n",
    "    assert not test.isna().any().any(), \"The test dataset has NaNs!\"\n",
    "\n",
    "    save(test, paths.test_set(ds))\n",
    "\n",
    "\n",
    "def make_train(ds, n, p, feats):\n",
    "    \"\"\"Creates a train dataset by joining features to one of the train candidate pairs set\"\"\"\n",
    "    cands = read(paths.train_candidate_pairs(ds, n, p))\n",
    "    train = merge_features(ds, cands.copy(), feats)\n",
    "\n",
    "    assert check_features(\n",
    "        train, cands\n",
    "    ), f\"The train dataset ({n}, {p}) is different from its candidates!\"\n",
    "    assert not train.isna().any().any(), f\"The train dataset ({n}, {p}) has NaNs!\"\n",
    "\n",
    "    save(train, paths.train_set(ds, n, p))\n",
    "\n",
    "\n",
    "def merge_features(ds, cands, feats):\n",
    "    \"\"\"Merges the features with the candidate pairs for a give dataset\"\"\"\n",
    "    # only keeps the relevant columns for the feature set\n",
    "    # score will later be used to truncate the test set and allow for\n",
    "    # evaluation on a different number of candidates\n",
    "    cands = cands[[\"dup_id\", \"candidate_id\", \"score\", \"is_dup\"]]\n",
    "    for f in feats:\n",
    "        df_feat = read(paths.feature(ds, f))\n",
    "        cands = cands.merge(df_feat, on=[\"candidate_id\", \"dup_id\"], how=\"left\")\n",
    "    return cands\n",
    "\n",
    "\n",
    "def make_train_multi(ds, ns, ps, feats):\n",
    "    \"\"\"Creates train sets of features for all combinations of candidate pairs and\n",
    "    undersampling percentages\n",
    "    \"\"\"\n",
    "    for n in ns:\n",
    "        for p in ps:\n",
    "            print(f\"- Making train set ({n}, {p})\")\n",
    "            make_train(ds, n, p, feats)\n",
    "\n",
    "\n",
    "def make_sets_multi(datasets, ns, ps, feats):\n",
    "    \"\"\"Creates train and test sets of features for all combinations of candidate pairs and\n",
    "    undersampling percentages for the given datasets\n",
    "    \"\"\"\n",
    "    for ds in datasets:\n",
    "        print(f\"Creating train and test sets for {ds}\")\n",
    "        make_train_multi(ds, ns, ps, feats)\n",
    "        make_test(ds, feats)\n",
    "\n",
    "\n",
    "def make_sets(datasets, n, p, feats):\n",
    "    \"\"\"Creates train and test sets of features for one value of candidate pairs and\n",
    "    undersampling percentages for the given datasets\n",
    "    \"\"\"\n",
    "    for ds in datasets:\n",
    "        print(f\"Creating train and test sets for {ds}\")\n",
    "        make_train(ds, n, p, feats)\n",
    "        make_test(ds, feats)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    make_sets_multi(datasets, n_candidates, undersampling_percentages, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7b1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for gamedev_se\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning hyperparameters for gamedev_so\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning hyperparameters for so_samples/sample_0\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning hyperparameters for so_samples/sample_1\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning hyperparameters for so_samples/sample_2\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning hyperparameters for so_samples/sample_3\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning hyperparameters for so_samples/sample_4\n",
      "- Tuning HPs for the train set with 1500 candidates, 0.01 percent dups\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import paths, read, save\n",
    "from utils.models.hp_tuning import tune_train_set\n",
    "from utils.consts import datasets, n_candidates, undersampling_percentages\n",
    "\n",
    "\n",
    "def random_forest_search(ds, c, p):\n",
    "    \"\"\"Tunes hyperparameters for the train set with c candidates and p undersampling percentage\n",
    "    for the given dataset using the provided hyperparameter tuning parameters\n",
    "    \"\"\"\n",
    "    train = read(paths.train_set(ds, c, p))\n",
    "    results = tune_train_set(train, 5)\n",
    "    results = pd.DataFrame(results.cv_results_)\n",
    "    save(results, paths.cv_results(ds, c, p))\n",
    "\n",
    "\n",
    "def tune_multiple_sets(datasets, ns, ps):\n",
    "    for ds in datasets:\n",
    "        print(f\"Tuning hyperparameters for {ds}\")\n",
    "        for p in ps:\n",
    "            for n in ns:\n",
    "                print(\n",
    "                    f\"- Tuning HPs for the train set with {n} candidates, {p} percent dups\"\n",
    "                )\n",
    "                rf = random_forest_search(ds, n, p)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tune_multiple_sets(datasets, n_candidates, undersampling_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374d72f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers for gamedev_se.\n",
      "- Training classifiers for 1500 candidates\n",
      "Training classifiers for gamedev_so.\n",
      "- Training classifiers for 1500 candidates\n",
      "Training classifiers for so_samples/sample_0.\n",
      "- Training classifiers for 1500 candidates\n",
      "Training classifiers for so_samples/sample_1.\n",
      "- Training classifiers for 1500 candidates\n",
      "Training classifiers for so_samples/sample_2.\n",
      "- Training classifiers for 1500 candidates\n",
      "Training classifiers for so_samples/sample_3.\n",
      "- Training classifiers for 1500 candidates\n",
      "Training classifiers for so_samples/sample_4.\n",
      "- Training classifiers for 1500 candidates\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  46.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=   0.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.5s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.5s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  47.0s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.6s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.3s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.3s\n",
      "- Selecting misclassified duplicates for gamedev_se\n",
      "- Selecting misclassified duplicates for gamedev_so\n",
      "- Selecting misclassified duplicates for so_samples/sample_0\n",
      "Evaluating classifiers on different candidates for gamedev_se\n",
      "- 1500 candidates\n",
      "Evaluating classifiers on different candidates for gamedev_so\n",
      "- 1500 candidates\n",
      "Evaluating classifiers on different candidates for so_samples/sample_0\n",
      "- 1500 candidates\n",
      "Evaluating classifiers on different candidates for so_samples/sample_1\n",
      "- 1500 candidates\n",
      "Evaluating classifiers on different candidates for so_samples/sample_2\n",
      "- 1500 candidates\n",
      "Evaluating classifiers on different candidates for so_samples/sample_3\n",
      "- 1500 candidates\n",
      "Evaluating classifiers on different candidates for so_samples/sample_4\n",
      "- 1500 candidates\n",
      "Evaluating classifiers cross-datasets\n",
      "- Training on: gamedev_se; testing on: gamedev_se\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  46.3s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=   0.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.6s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.3s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  46.4s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.4s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.5s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.3s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  46.3s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=   0.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.5s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.6s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=   0.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.6s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=   0.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.5s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.3s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.2s\n",
      "[CV] END bootstrap=False, class_weight=None, max_depth=8, min_samples_leaf=2, min_samples_split=10, n_estimators=205, random_state=42; total time=  23.3s\n",
      "- Training on: gamedev_se; testing on: gamedev_so\n",
      "- Training on: gamedev_se; testing on: so_samples/sample_0\n",
      "- Training on: gamedev_se; testing on: so_samples/sample_1\n",
      "- Training on: gamedev_se; testing on: so_samples/sample_2\n",
      "- Training on: gamedev_se; testing on: so_samples/sample_3\n",
      "- Training on: gamedev_se; testing on: so_samples/sample_4\n",
      "- Training on: gamedev_so; testing on: gamedev_se\n",
      "- Training on: gamedev_so; testing on: gamedev_so\n",
      "- Training on: gamedev_so; testing on: so_samples/sample_0\n",
      "- Training on: gamedev_so; testing on: so_samples/sample_1\n",
      "- Training on: gamedev_so; testing on: so_samples/sample_2\n",
      "- Training on: gamedev_so; testing on: so_samples/sample_3\n",
      "- Training on: gamedev_so; testing on: so_samples/sample_4\n",
      "- Training on: so_samples/sample_0; testing on: gamedev_se\n",
      "- Training on: so_samples/sample_0; testing on: gamedev_so\n",
      "- Training on: so_samples/sample_0; testing on: so_samples/sample_0\n",
      "- Training on: so_samples/sample_0; testing on: so_samples/sample_1\n",
      "- Training on: so_samples/sample_0; testing on: so_samples/sample_2\n",
      "- Training on: so_samples/sample_0; testing on: so_samples/sample_3\n",
      "- Training on: so_samples/sample_0; testing on: so_samples/sample_4\n",
      "- Training on: so_samples/sample_1; testing on: gamedev_se\n",
      "- Training on: so_samples/sample_1; testing on: gamedev_so\n",
      "- Training on: so_samples/sample_1; testing on: so_samples/sample_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training on: so_samples/sample_1; testing on: so_samples/sample_1\n",
      "- Training on: so_samples/sample_1; testing on: so_samples/sample_2\n",
      "- Training on: so_samples/sample_1; testing on: so_samples/sample_3\n",
      "- Training on: so_samples/sample_1; testing on: so_samples/sample_4\n",
      "- Training on: so_samples/sample_2; testing on: gamedev_se\n",
      "- Training on: so_samples/sample_2; testing on: gamedev_so\n",
      "- Training on: so_samples/sample_2; testing on: so_samples/sample_0\n",
      "- Training on: so_samples/sample_2; testing on: so_samples/sample_1\n",
      "- Training on: so_samples/sample_2; testing on: so_samples/sample_2\n",
      "- Training on: so_samples/sample_2; testing on: so_samples/sample_3\n",
      "- Training on: so_samples/sample_2; testing on: so_samples/sample_4\n",
      "- Training on: so_samples/sample_3; testing on: gamedev_se\n",
      "- Training on: so_samples/sample_3; testing on: gamedev_so\n",
      "- Training on: so_samples/sample_3; testing on: so_samples/sample_0\n",
      "- Training on: so_samples/sample_3; testing on: so_samples/sample_1\n",
      "- Training on: so_samples/sample_3; testing on: so_samples/sample_2\n",
      "- Training on: so_samples/sample_3; testing on: so_samples/sample_3\n",
      "- Training on: so_samples/sample_3; testing on: so_samples/sample_4\n",
      "- Training on: so_samples/sample_4; testing on: gamedev_se\n",
      "- Training on: so_samples/sample_4; testing on: gamedev_so\n",
      "- Training on: so_samples/sample_4; testing on: so_samples/sample_0\n",
      "- Training on: so_samples/sample_4; testing on: so_samples/sample_1\n",
      "- Training on: so_samples/sample_4; testing on: so_samples/sample_2\n",
      "- Training on: so_samples/sample_4; testing on: so_samples/sample_3\n",
      "- Training on: so_samples/sample_4; testing on: so_samples/sample_4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from utils import paths, read, save, make_dir\n",
    "from utils.models import get_X_y\n",
    "from utils.models.scoring import multiple_k_scorer, predict_probabilities\n",
    "from utils.consts import (\n",
    "    datasets,\n",
    "    n_procs,\n",
    "    n_candidates,\n",
    "    undersampling_percentages,\n",
    "    best_candidates,\n",
    "    best_undersampling,\n",
    ")\n",
    "\n",
    "\n",
    "def best_classifier(ds, c, p):\n",
    "    \"\"\"Reads the HP tuning results and returns the classifier\n",
    "    that achieved the highest score\n",
    "    \"\"\"\n",
    "    df = read(paths.cv_results(ds, c, p))\n",
    "    # manual analysis suggests that rr@5 was a good metric\n",
    "    # for choosing the best classifier\n",
    "    params = df[df[\"rank_test_rr@5\"] == 1].iloc[0].params\n",
    "\n",
    "    # the dict objects in the dataframe\n",
    "    # are sometimes converted to json format\n",
    "    if type(params) == bytes:\n",
    "        params = json.loads(params)\n",
    "\n",
    "    rf = RandomForestClassifier(n_jobs=n_procs, **params)\n",
    "    return rf\n",
    "\n",
    "\n",
    "def train_best_classifier(ds, c, p):\n",
    "    \"\"\"Fits the best classifier from HP tuning on the train set\n",
    "    for the given dataset and saves it\n",
    "    \"\"\"\n",
    "    rf = best_classifier(ds, c, p)\n",
    "    train = read(paths.train_set(ds, c, p))\n",
    "    X, y = get_X_y(train)\n",
    "    rf = rf.fit(X, y)\n",
    "\n",
    "    make_dir(paths.classifiers_dir(ds))\n",
    "    joblib.dump(rf, paths.classifier(ds, c, p))\n",
    "\n",
    "    \n",
    "def limit_candidates(df, c):\n",
    "    df = df.groupby(\"dup_id\").apply(\n",
    "        lambda x: x.sort_values(\"score\", ascending=False)[:c]\n",
    "    )\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def score(ds, c, p, test_on=None):\n",
    "    \"\"\"Scores a given dataset by training on its train set and\n",
    "    evaluating on the test set.\n",
    "    Allows for using other datasets for evaluation using the test_on param\n",
    "    \"\"\"\n",
    "    if test_on is None:\n",
    "        dataset_test = ds\n",
    "    else:\n",
    "        dataset_test = test_on\n",
    "\n",
    "    rf = joblib.load(paths.classifier(ds, c, p))\n",
    "\n",
    "    test = read(paths.test_set(dataset_test))\n",
    "    test = limit_candidates(test, c)\n",
    "    X, y = get_X_y(test)\n",
    "\n",
    "    scores = multiple_k_scorer(rf, X, y)\n",
    "\n",
    "    scores[\"dataset_train\"] = ds\n",
    "    scores[\"candidates\"] = c\n",
    "\n",
    "    if test_on is not None:\n",
    "        scores[\"dataset_test\"] = test_on\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def candidate_performance(datasets, ns, p):\n",
    "    \"\"\"Tests every dataset on its own test set using\n",
    "    different numbers of candidates and saves a summary dataset\"\"\"\n",
    "    df = []\n",
    "    for ds in datasets:\n",
    "        print(f\"Evaluating classifiers on different candidates for {ds}\")\n",
    "        for n in ns:\n",
    "            print(f\"- {n} candidates\")\n",
    "            df.append(score(ds, n, p))\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    save(df, paths.candidates_evaluation())\n",
    "\n",
    "\n",
    "def cross_dataset_performance(datasets, n, p):\n",
    "    \"\"\"Evaluates the performance of classifiers trained on one dataset in using\n",
    "    the others as evaluation\"\"\"\n",
    "    df = []\n",
    "\n",
    "    print(f\"Evaluating classifiers cross-datasets\")\n",
    "\n",
    "    for ds1 in datasets:\n",
    "        for ds2 in datasets:\n",
    "            print(f\"- Training on: {ds1}; testing on: {ds2}\")\n",
    "            df.append(score(ds1, n, p, test_on=ds2))\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df = df[[\"dataset_train\", \"dataset_test\", \"rr@5\", \"rr@10\", \"rr@20\"]]\n",
    "    df = df.sort_values('dataset_test')\n",
    "\n",
    "    save(df, paths.cross_dataset_performance())\n",
    "    \n",
    "def misclassified_dups(ds, c, p):\n",
    "    \"\"\"Makes a dataset of the duplicate questions that do not have their main question\n",
    "    in the 20 top ranked pairs for the dataset\"\"\"\n",
    "    def select_misclassified(df, dups):\n",
    "        \"\"\"Selects the true pair and the top ranked pair for each misclassified duplicate\"\"\"\n",
    "        dup = df.name\n",
    "        df = df.sort_values('pred', ascending=False)\n",
    "        if df[:20]['is_dup'].any():\n",
    "            # correctly classified = nothing to return\n",
    "            res = pd.DataFrame()\n",
    "        else:\n",
    "            # if the dup is in the list of candidates at all\n",
    "            has_dup = df.is_dup.any()\n",
    "            \n",
    "            if has_dup:\n",
    "                true_dup = df[df.is_dup].iloc[0]['candidate_id']\n",
    "            else:\n",
    "                true_dup = dups[dups.dup_id == dup].iloc[0]['main_id']\n",
    "                \n",
    "            top_ranked = df[~df.is_dup].iloc[0]['candidate_id']\n",
    "            \n",
    "            res = pd.DataFrame([{'main_id': true_dup, 'top_ranked': top_ranked, 'has_dup': has_dup}])\n",
    "        return res\n",
    "\n",
    "    print(f'- Selecting misclassified duplicates for {ds}')\n",
    "        \n",
    "    rf = joblib.load(paths.classifier(ds, c, p))\n",
    "    \n",
    "    test = read(paths.test_set(ds))\n",
    "    test = limit_candidates(test, c)\n",
    "    X, y = get_X_y(test)\n",
    "    \n",
    "    test = predict_probabilities(rf, X, y)\n",
    "    \n",
    "    dup_pairs = read(paths.dup_pairs(ds))\n",
    "    \n",
    "    missed = test.groupby('dup_id').apply(lambda df: select_misclassified(df, dup_pairs))\n",
    "\n",
    "    if len(missed) > 0:\n",
    "        missed = missed.reset_index()\n",
    "        missed['has_dup'] = missed.has_dup.apply(bool)\n",
    "        \n",
    "        missed = missed.drop(columns=['level_1'])\n",
    "        \n",
    "        # adds URLs for easier analysis\n",
    "        if ds == 'gamedev_se':\n",
    "            to_url = lambda i: f'https://gamedev.stackexchange.com/questions/{i}/'\n",
    "        else:\n",
    "            to_url = lambda i: f'https://stackoverflow.com/questions/{i}/'\n",
    "        \n",
    "        for c in [c for c in missed.columns if c != 'has_dup']:\n",
    "            missed[c + '_url'] = missed[c].apply(to_url)\n",
    "        \n",
    "        if 'gamedev' not in ds:\n",
    "            ds = 'so_sample'\n",
    "        \n",
    "    save(missed, paths.misclassified_duplicates(ds))\n",
    "\n",
    "\n",
    "def train_classifiers(ds, ns, ps):\n",
    "    \"\"\"Trains classifiers for all combinations of candidates and undersampling\n",
    "    percentages\n",
    "    \"\"\"\n",
    "    print(f\"Training classifiers for {ds}.\")\n",
    "    for n in ns:\n",
    "        for p in ps:\n",
    "            print(f\"- Training classifiers for {n} candidates\")\n",
    "            train_best_classifier(ds, n, p)\n",
    "\n",
    "\n",
    "def main(datasets, ns, ps, best_n, best_p):\n",
    "    for ds in datasets:\n",
    "        train_classifiers(ds, ns, ps)\n",
    "\n",
    "    for ds in ['gamedev_se', 'gamedev_so', 'so_samples/sample_0']:\n",
    "        misclassified_dups(ds, best_n, best_p)\n",
    "\n",
    "    candidate_performance(datasets, ns, best_p)\n",
    "    cross_dataset_performance(datasets, best_n, best_p)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        datasets,\n",
    "        n_candidates,\n",
    "        undersampling_percentages,\n",
    "        best_candidates,\n",
    "        best_undersampling,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c875fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def root_dir():\n",
    "    path = os.path.abspath(os.getcwd())\n",
    "    root = path.rsplit(\"code\", 1)[0]\n",
    "    return Path(root)\n",
    "\n",
    "\n",
    "def percent_to_string(p):\n",
    "    p = p * 100\n",
    "    if p > 0:\n",
    "        p = int(p)\n",
    "    return str(p).replace(\".\", \"_\")\n",
    "\n",
    "\n",
    "####################\n",
    "## Data dirs      ##\n",
    "####################\n",
    "\n",
    "\n",
    "def data_dir():\n",
    "    return root_dir() / \"data\"\n",
    "\n",
    "\n",
    "def dataset_dir(ds):\n",
    "    return data_dir() / ds\n",
    "\n",
    "\n",
    "# Raw data\n",
    "\n",
    "\n",
    "def raw_dir(ds):\n",
    "    return dataset_dir(ds) / \"raw\"\n",
    "\n",
    "\n",
    "def posts_xml(ds, i=None):\n",
    "    return raw_dir(ds) / \"Posts.xml\"\n",
    "\n",
    "\n",
    "def post_links_xml(ds):\n",
    "    return raw_dir(ds) / \"PostLinks.xml\"\n",
    "\n",
    "\n",
    "def questions_xml(ds, i=None):\n",
    "    if i is None:\n",
    "        return raw_dir(ds) / \"questions.xml\"\n",
    "    else:\n",
    "        return raw_dir(ds) / f\"questions_{i}.xml\"\n",
    "\n",
    "\n",
    "def answers_xml(ds, i=None):\n",
    "    if i is None:\n",
    "        return raw_dir(ds) / \"answers.xml\"\n",
    "    else:\n",
    "        return raw_dir(ds) / f\"answers_{i}.xml\"\n",
    "\n",
    "\n",
    "# Question IDs\n",
    "\n",
    "\n",
    "def ids_dir(ds):\n",
    "    return dataset_dir(ds) / \"question_ids\"\n",
    "\n",
    "\n",
    "def all_question_ids(ds):\n",
    "    return ids_dir(ds) / \"all_question_ids.parquet\"\n",
    "\n",
    "\n",
    "def accepted_answer_ids(ds):\n",
    "    return ids_dir(ds) / \"accepted_answers.parquet\"\n",
    "\n",
    "\n",
    "def dup_pairs(ds):\n",
    "    return ids_dir(ds) / \"dup_pairs.parquet\"\n",
    "\n",
    "\n",
    "def duplicate_question_ids(ds):\n",
    "    return ids_dir(ds) / \"duplicate_question_ids.parquet\"\n",
    "\n",
    "\n",
    "def main_question_ids(ds):\n",
    "    return ids_dir(ds) / \"main_question_ids.parquet\"\n",
    "\n",
    "\n",
    "def noise_question_ids(ds):\n",
    "    return ids_dir(ds) / \"noise_question_ids.parquet\"\n",
    "\n",
    "\n",
    "def comparison_question_ids(ds):\n",
    "    return ids_dir(ds) / \"comparison_question_ids.parquet\"\n",
    "\n",
    "\n",
    "def answered_question_ids(ds):\n",
    "    return ids_dir(ds) / \"answered_question_ids.parquet\"\n",
    "\n",
    "\n",
    "def train_dup_ids(ds):\n",
    "    return ids_dir(ds) / \"train_dup_ids.parquet\"\n",
    "\n",
    "\n",
    "def test_dup_ids(ds):\n",
    "    return ids_dir(ds) / \"test_dup_ids.parquet\"\n",
    "\n",
    "\n",
    "# Corpus\n",
    "\n",
    "\n",
    "def corpus_dir(ds):\n",
    "    return dataset_dir(ds) / \"corpus\"\n",
    "\n",
    "\n",
    "def question_texts(ds, i=None):\n",
    "    if i is None:\n",
    "        return corpus_dir(ds) / \"question_texts.parquet\"\n",
    "    else:\n",
    "        return corpus_dir(ds) / f\"question_texts_{i}.parquet\"\n",
    "\n",
    "\n",
    "def answer_texts(ds, i=None):\n",
    "    if i is None:\n",
    "        return corpus_dir(ds) / \"answer_texts.parquet\"\n",
    "    else:\n",
    "        return corpus_dir(ds) / f\"answer_texts_{i}.parquet\"\n",
    "\n",
    "\n",
    "def corpus(ds, tokenized=True):\n",
    "    if tokenized:\n",
    "        return corpus_dir(ds) / \"corpus_tokenized.parquet\"\n",
    "    else:\n",
    "        return corpus_dir(ds) / \"corpus.parquet\"\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "\n",
    "\n",
    "def embeddings_dir(ds):\n",
    "    return dataset_dir(ds) / \"embeddings\"\n",
    "\n",
    "\n",
    "def embedding_dir(ds, m):\n",
    "    return embeddings_dir(ds) / m\n",
    "\n",
    "\n",
    "def embedding(ds, m, c):\n",
    "    return embedding_dir(ds, m) / f\"{c}.{m}.npz\"\n",
    "\n",
    "\n",
    "# Features\n",
    "\n",
    "\n",
    "def features_dir(ds):\n",
    "    return dataset_dir(ds) / \"features\"\n",
    "\n",
    "\n",
    "def feature(ds, f, i=None):\n",
    "    if i is None:\n",
    "        save = features_dir(ds) / f\"{f}.parquet\"\n",
    "    else:\n",
    "        save = features_dir(ds) / f / f\"{i}.parquet\"\n",
    "    return save\n",
    "\n",
    "\n",
    "# Candidates\n",
    "\n",
    "\n",
    "def candidate_pairs_dir(ds):\n",
    "    return ids_dir(ds) / \"candidate_pairs\"\n",
    "\n",
    "\n",
    "def candidate_pairs(ds, i=None):\n",
    "    if i is None:\n",
    "        path = candidate_pairs_dir(ds) / \"candidate_pairs.parquet\"\n",
    "    else:\n",
    "        path = candidate_pairs_dir(ds) / \"split\" / f\"candidate_pairs_{i}.parquet\"\n",
    "    return path\n",
    "\n",
    "\n",
    "def train_candidate_pairs(ds, c, p=None, i=None):\n",
    "    filename = f\"train_candidate_pairs_{c}_candidates\"\n",
    "\n",
    "    if p is not None:\n",
    "        p = percent_to_string(p)\n",
    "        filename += f\"_{p}_perc_dups\"\n",
    "    if i is not None:\n",
    "        filename += f\"_{i}\"\n",
    "\n",
    "    filename += \".parquet\"\n",
    "\n",
    "    return candidate_pairs_dir(ds) / filename\n",
    "\n",
    "\n",
    "def test_candidate_pairs(ds):\n",
    "    return candidate_pairs_dir(ds) / \"test_candidate_pairs.parquet\"\n",
    "\n",
    "\n",
    "# Train/test sets\n",
    "\n",
    "\n",
    "def train_sets_dir(ds):\n",
    "    return dataset_dir(ds) / \"train_sets\"\n",
    "\n",
    "\n",
    "def test_sets_dir(ds):\n",
    "    return dataset_dir(ds) / \"test_sets\"\n",
    "\n",
    "\n",
    "def train_set(ds, c, p, i=None):\n",
    "    p = percent_to_string(p)\n",
    "    return train_sets_dir(ds) / f\"train_{c}_candidates_{p}_perc_dups.parquet\"\n",
    "\n",
    "\n",
    "def test_set(ds):\n",
    "    return test_sets_dir(ds) / f\"test.parquet\"\n",
    "\n",
    "\n",
    "# Cross-val results\n",
    "\n",
    "\n",
    "def cv_results_dir(ds):\n",
    "    return dataset_dir(ds) / \"cv_results\"\n",
    "\n",
    "\n",
    "def cv_results(ds, c, p):\n",
    "    p = percent_to_string(p)\n",
    "    return cv_results_dir(ds) / f\"cv_results_{c}_candidates_{p}_perc_dups.parquet\"\n",
    "\n",
    "\n",
    "####################\n",
    "## Model dirs     ##\n",
    "####################\n",
    "\n",
    "\n",
    "def models_dir():\n",
    "    return root_dir() / \"models\"\n",
    "\n",
    "\n",
    "def dataset_models_dir(ds):\n",
    "    return models_dir() / ds\n",
    "\n",
    "\n",
    "# Feature models\n",
    "\n",
    "\n",
    "def feature_model_dir(ds, m):\n",
    "    return dataset_models_dir(ds) / m\n",
    "\n",
    "\n",
    "def feature_model(ds, m, c):\n",
    "    return feature_model_dir(ds, m) / f\"{c}.{m}\"\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "\n",
    "\n",
    "def classifiers_dir(ds):\n",
    "    return dataset_models_dir(ds) / \"classifiers\"\n",
    "\n",
    "\n",
    "def classifier(ds, c, p):\n",
    "    p = percent_to_string(p)\n",
    "    return classifiers_dir(ds) / f\"classifier_{c}_candidates_{p}_perc_dups.joblib\"\n",
    "\n",
    "\n",
    "####################\n",
    "## Analysis dirs  ##\n",
    "####################\n",
    "\n",
    "\n",
    "def analysis_dir():\n",
    "    return data_dir() / \"analysis\"\n",
    "\n",
    "\n",
    "def analysis_file(f):\n",
    "    return analysis_dir() / f\n",
    "\n",
    "\n",
    "def candidates_evaluation():\n",
    "    return analysis_file(\"candidates_evaluation.parquet\")\n",
    "\n",
    "\n",
    "def cross_dataset_performance():\n",
    "    return analysis_file(\"cross_dataset_performance.parquet\")\n",
    "\n",
    "\n",
    "# Misclassified duplicates\n",
    "\n",
    "\n",
    "def misclassified_duplicates_dir():\n",
    "    return analysis_dir() / \"misclassified_duplicates\"\n",
    "\n",
    "\n",
    "def misclassified_duplicates(ds):\n",
    "    return misclassified_duplicates_dir() / f\"{ds}_misclassified.parquet\"\n",
    "\n",
    "\n",
    "# Question pair ranks\n",
    "\n",
    "\n",
    "def pair_ranks_dir():\n",
    "    return analysis_dir() / \"duplicate_pair_ranks\"\n",
    "\n",
    "\n",
    "def pair_ranks_dataset_dir(ds):\n",
    "    return pair_ranks_dir() / ds\n",
    "\n",
    "\n",
    "def pair_ranks(ds, m, c):\n",
    "    return pair_ranks_dataset_dir(ds) / f\"{m}_{c}.parquet\"\n",
    "\n",
    "\n",
    "def all_pair_ranks(ds):\n",
    "    return pair_ranks_dataset_dir(ds) / \"all_pair_ranks.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9448c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# tags used for selecting gamedev questions on Stack Overflow\n",
    "gamedev_tags = [\n",
    "    \"game-engine\",\n",
    "    \"game-physics\",\n",
    "    \"game-development\",\n",
    "    \"gameobject\",\n",
    "    \"2d-games\",\n",
    "    \"unreal-engine4\",\n",
    "    \"unreal-blueprint\",\n",
    "    \"unreal-development-kit\",\n",
    "    \"unrealscript\",\n",
    "    \"unity3d\",\n",
    "    \"unity5\",\n",
    "    \"unity5.3\",\n",
    "    \"unity3d-mecanim\",\n",
    "    \"unity3d-terrain\",\n",
    "    \"unityscript\",\n",
    "    \"unity3d-2dtools\",\n",
    "    \"unity3d-unet\",\n",
    "    \"unity-webgl\",\n",
    "    \"unity2d\",\n",
    "    \"unity-editor\",\n",
    "    \"unity3d-editor\",\n",
    "    \"unity-networking\",\n",
    "    \"unity3d-gui\",\n",
    "    \"unity-ui\",\n",
    "    \"unity3d-5\",\n",
    "]\n",
    "\n",
    "# Seeds used for sampling Stack Overflow questions\n",
    "so_sample_seeds = [5129, 1011, 3692, 2420, 5815]\n",
    "\n",
    "# names of the Stack Overflow samples\n",
    "so_samples = [f\"so_samples/sample_{i}\" for i, _ in enumerate(so_sample_seeds)]\n",
    "\n",
    "gamedev_datasets = [\n",
    "    \"gamedev_se\",\n",
    "    \"gamedev_so\",\n",
    "]\n",
    "\n",
    "datasets = gamedev_datasets + so_samples\n",
    "\n",
    "# Similarity measures / features used in the classifiers\n",
    "features = [\n",
    "    \"jaccard\",\n",
    "    \"tfidf\",\n",
    "    \"bm25\",\n",
    "    \"topic\",\n",
    "    \"doc2vec\",\n",
    "    #'bertoverflow',\n",
    "    #'mpnet'        \n",
    "]\n",
    "\n",
    "text_columns = [\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"title_body\",\n",
    "    \"title_body_tags\",\n",
    "    \"title_body_tags_answer\",\n",
    "]\n",
    "\n",
    "# number of CPU cores to use\n",
    "n_procs = 1\n",
    "\n",
    "# percentage of fake duplicate questions in the train set\n",
    "noise_percentage = 0.2\n",
    "\n",
    "# number of candidates used for training and evaluating classifiers\n",
    "n_candidates = [1500]\n",
    "\n",
    "# percentages used for undersampling the train sed\n",
    "undersampling_percentages = [0.01]\n",
    "\n",
    "# percentage of the train/test split\n",
    "split_percentage = 0.2\n",
    "\n",
    "# best values we found for the undersampling / number of selected candidates\n",
    "best_undersampling = 0.01\n",
    "best_candidates = 1500\n",
    "\n",
    "# number of iterations used during random HP tuning\n",
    "search_n_iters = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0261b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "read = pd.read_parquet\n",
    "\n",
    "\n",
    "def save(df, path):\n",
    "    if not path.exists():\n",
    "        make_dir(path.parent)\n",
    "    df.to_parquet(path)\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e679c23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27990/1359577993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBM25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.special import rel_entr\n",
    "from scipy.spatial.distance import cdist\n",
    "from gensim.matutils import jaccard_distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from . import paths as paths\n",
    "from . import read, save\n",
    "from .models.bm25 import BM25\n",
    "\n",
    "\n",
    "class QuestionComp:\n",
    "    \"\"\"Class for comparing sets of questions using different similarity measures\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, model_name, column):\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.column = column\n",
    "        self.embeddings = None\n",
    "        self.load_embedding()\n",
    "\n",
    "    def load_embedding(self):\n",
    "        \"\"\"Loads the document representations for a dataset/model/column\n",
    "        using an appropriate method\n",
    "        \"\"\"\n",
    "        if self.model_name == \"bm25\":\n",
    "            emb = BM25.load(\n",
    "                paths.feature_model(self.dataset, self.model_name, self.column)\n",
    "            )\n",
    "        elif self.model_name == \"jaccard\":\n",
    "            # creates sets of words\n",
    "            emb = read(paths.corpus(self.dataset))[[self.column, \"corpus_index\"]]\n",
    "            emb = emb.set_index(\"corpus_index\")[self.column].apply(set)\n",
    "        else:\n",
    "            emb = load_npz(paths.embedding(self.dataset, self.model_name, self.column))\n",
    "\n",
    "            # lda embeddings are not arrays\n",
    "            if self.model_name == \"lda\":\n",
    "                emb = emb.toarray()\n",
    "        self.embeddings = emb\n",
    "\n",
    "    def topic_sim(self, indexes, others):\n",
    "        \"\"\"Calculates topic similarity between sets of documents represented by indexes\n",
    "        Each index indicates the position of the document in the corpus\n",
    "        and serves to select its representation in the embedding matrix\n",
    "        \"\"\"\n",
    "        embedding = self.embeddings[indexes]\n",
    "        other_embeddings = self.embeddings[others]\n",
    "        res = cdist(embedding, other_embeddings, metric=\"jensenshannon\")\n",
    "        # converting distance to similarity\n",
    "        res = np.negative(res)\n",
    "        res = np.add(1, res)\n",
    "        return res\n",
    "\n",
    "    def bm25_sim(self, indexes, others):\n",
    "        \"\"\"Calculates BM25 scores between sets of documents represented by indexes\n",
    "        Each index indicates the position of the document in the corpus\n",
    "        and serves to select its representation in the corpus representation\n",
    "        in the BM25 class\n",
    "        \"\"\"\n",
    "\n",
    "        def compare(i):\n",
    "            return self.embeddings.compare_documents(i, others)\n",
    "\n",
    "        return [compare(i) for i in indexes]\n",
    "\n",
    "    def jac_sim(self, indexes, others):\n",
    "        \"\"\"Calculates Jaccard similarities between sets of documents represented by indexes\n",
    "        Each index indicates the position of the document in the corpus\n",
    "        and serves to select the corresponding set of words\n",
    "        \"\"\"\n",
    "\n",
    "        def compare(i):\n",
    "            embedding = self.embeddings.loc[i]\n",
    "            other_embeddings = self.embeddings.loc[others]\n",
    "            return other_embeddings.apply(\n",
    "                lambda t: 1.0 - jaccard_distance(t, embedding)\n",
    "            ).to_list()\n",
    "\n",
    "        return [compare(i) for i in indexes]\n",
    "\n",
    "    def cosine_sim(self, indexes, others):\n",
    "        \"\"\"Calculates cosine similarity between sets of documents represented by indexes\n",
    "        Each index indicates the position of the document in the corpus\n",
    "        and serves to select its representation in the embedding matrix\n",
    "        This function is used for TF-IDF, Doc2Vec, MPNet and BertOverflow similarities\n",
    "        \"\"\"\n",
    "        embedding = self.embeddings[indexes]\n",
    "        other_embeddings = self.embeddings[others]\n",
    "        return cosine_similarity(embedding, other_embeddings)\n",
    "\n",
    "    def compare(self, indexes, others):\n",
    "        \"\"\"Compares sets of documents represented by indexes using the\n",
    "        appropriate similarity function\n",
    "        \"\"\"\n",
    "        # allows for passing a single index instead of a list of indexes\n",
    "        single_index = type(indexes) in (int, np.int64)\n",
    "        single_comp = type(others) in (int, np.int64)\n",
    "        if single_index:\n",
    "            indexes = [indexes]\n",
    "        if single_comp:\n",
    "            others = [others]\n",
    "\n",
    "        if self.model_name == \"jaccard\":\n",
    "            scores = self.jac_sim(indexes, others)\n",
    "        elif self.model_name == \"bm25\":\n",
    "            scores = self.bm25_sim(indexes, others)\n",
    "        elif self.model_name == \"lda\":\n",
    "            scores = self.topic_sim(indexes, others)\n",
    "        else:\n",
    "            scores = self.cosine_sim(indexes, others)\n",
    "\n",
    "        # return a single value/array instead of a matrix\n",
    "        if single_index:\n",
    "            scores = scores[0]\n",
    "            if single_comp:\n",
    "                scores = scores[0]\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0127e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n",
    "\n",
    "\"\"\"Modified implementation of the BM25 algorithm provided in Gensim 3.8.\n",
    "https://github.com/RaRe-Technologies/gensim/blob/3.8.3/gensim/summarization/bm25.py\n",
    "\n",
    "Modifications are mostly for performance increases and new functions for computing\n",
    "scores for arbitrary indexes in the corpus\n",
    "\n",
    "Also added functions to save and load the model\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"This module contains function of computing rank scores for documents in\n",
    "corpus and helper class `BM25` used in calculations. Original algorithm\n",
    "descibed in [1]_, also you may check Wikipedia page [2]_.\n",
    ".. [1] Robertson, Stephen; Zaragoza, Hugo (2009).  The Probabilistic Relevance Framework: BM25 and Beyond,\n",
    "       http://www.staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf\n",
    ".. [2] Okapi BM25 on Wikipedia, https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "Examples\n",
    "--------\n",
    ".. sourcecode:: pycon\n",
    "    >>> from gensim.summarization.bm25 import get_bm25_weights\n",
    "    >>> corpus = [\n",
    "    ...     [\"black\", \"cat\", \"white\", \"cat\"],\n",
    "    ...     [\"cat\", \"outer\", \"space\"],\n",
    "    ...     [\"wag\", \"dog\"]\n",
    "    ... ]\n",
    "    >>> result = get_bm25_weights(corpus, n_jobs=-1)\n",
    "Data:\n",
    "-----\n",
    ".. data:: PARAM_K1 - Free smoothing parameter for BM25.\n",
    ".. data:: PARAM_B - Free smoothing parameter for BM25.\n",
    ".. data:: EPSILON - Constant used for negative idf of document in corpus.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import joblib\n",
    "from six import iteritems\n",
    "from six.moves import range\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "PARAM_K1 = 1.5\n",
    "PARAM_B = 0.75\n",
    "EPSILON = 0.25\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def effective_n_jobs(n_jobs):\n",
    "    \"\"\"Determines the number of jobs can run in parallel.\n",
    "    Just like in sklearn, passing n_jobs=-1 means using all available\n",
    "    CPU cores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_jobs : int\n",
    "        Number of workers requested by caller.\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Number of effective jobs.\n",
    "    \"\"\"\n",
    "    if n_jobs == 0:\n",
    "        raise ValueError(\"n_jobs == 0 in Parallel has no meaning\")\n",
    "    elif n_jobs is None:\n",
    "        return 1\n",
    "    elif n_jobs < 0:\n",
    "        n_jobs = max(cpu_count() + 1 + n_jobs, 1)\n",
    "    return n_jobs\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    \"\"\"Implementation of Best Matching 25 ranking function.\n",
    "    Attributes\n",
    "    ----------\n",
    "    corpus_size : int\n",
    "        Size of corpus (number of documents).\n",
    "    avgdl : float\n",
    "        Average length of document in `corpus`.\n",
    "    doc_freqs : list of dicts of int\n",
    "        Dictionary with terms frequencies for each document in `corpus`. Words used as keys and frequencies as values.\n",
    "    idf : dict\n",
    "        Dictionary with inversed documents frequencies for whole `corpus`. Words used as keys and frequencies as values.\n",
    "    doc_len : list of int\n",
    "        List of document lengths.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus=None, k1=PARAM_K1, b=PARAM_B, epsilon=EPSILON):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : list of list of str\n",
    "            Given corpus.\n",
    "        k1 : float\n",
    "            Constant used for influencing the term frequency saturation. After saturation is reached, additional\n",
    "            presence for the term adds a significantly less additional score. According to [1]_, experiments suggest\n",
    "            that 1.2 < k1 < 2 yields reasonably good results, although the optimal value depends on factors such as\n",
    "            the type of documents or queries.\n",
    "        b : float\n",
    "            Constant used for influencing the effects of different document lengths relative to average document length.\n",
    "            When b is bigger, lengthier documents (compared to average) have more impact on its effect. According to\n",
    "            [1]_, experiments suggest that 0.5 < b < 0.8 yields reasonably good results, although the optimal value\n",
    "            depends on factors such as the type of documents or queries.\n",
    "        epsilon : float\n",
    "            Constant used as floor value for idf of a document in the corpus. When epsilon is positive, it restricts\n",
    "            negative idf values. Negative idf implies that adding a very common term to a document penalize the overall\n",
    "            score (with 'very common' meaning that it is present in more than half of the documents). That can be\n",
    "            undesirable as it means that an identical document would score less than an almost identical one (by\n",
    "            removing the referred term). Increasing epsilon above 0 raises the sense of how rare a word has to be (among\n",
    "            different documents) to receive an extra score.\n",
    "        \"\"\"\n",
    "\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.corpus_size = 0\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "\n",
    "        if corpus is not None:\n",
    "            self._initialize(corpus)\n",
    "\n",
    "    def load(path):\n",
    "        bm25 = BM25()\n",
    "        bm25.__dict__ = joblib.load(path)\n",
    "        return bm25\n",
    "\n",
    "    def save(self, path):\n",
    "        joblib.dump(self.__dict__, path)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        \"\"\"Calculates frequencies of terms in documents and in corpus. Also computes inverse document frequencies.\"\"\"\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.corpus_size += 1\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                if word not in nd:\n",
    "                    nd[word] = 0\n",
    "                nd[word] += 1\n",
    "\n",
    "        self.avgdl = float(num_doc) / self.corpus_size\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "        negative_idfs = []\n",
    "        for word, freq in iteritems(nd):\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = float(idf_sum) / len(self.idf)\n",
    "\n",
    "        if self.average_idf < 0:\n",
    "            logger.warning(\n",
    "                \"Average inverse document frequency is less than zero. Your corpus of {} documents\"\n",
    "                \" is either too small or it does not originate from natural text. BM25 may produce\"\n",
    "                \" unintuitive results.\".format(self.corpus_size)\n",
    "            )\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "        doc_den = [\n",
    "            self.k1 * (1 - self.b + self.b * dl / self.avgdl) for dl in self.doc_len\n",
    "        ]\n",
    "        num_constant = self.k1 + 1\n",
    "\n",
    "        self.doc_scores = []\n",
    "\n",
    "        for i, doc in enumerate(self.doc_freqs):\n",
    "            self.doc_scores.append(\n",
    "                {\n",
    "                    w: df * self.idf[w] * num_constant / (df + doc_den[i])\n",
    "                    for w, df in doc.items()\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def get_score(self, document, index):\n",
    "        \"\"\"Computes BM25 score of given `document` in relation to item of corpus selected by `index`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : list of str\n",
    "            Document to be scored.\n",
    "        index : int\n",
    "            Index of document in corpus selected to score with `document`.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            BM25 score.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        doc_freqs = self.doc_freqs[index]\n",
    "        numerator_constant = self.k1 + 1\n",
    "        denominator_constant = self.k1 * (\n",
    "            1 - self.b + self.b * self.doc_len[index] / self.avgdl\n",
    "        )\n",
    "        for word in document:\n",
    "            if word in doc_freqs:\n",
    "                df = self.doc_freqs[index][word]\n",
    "                idf = self.idf[word]\n",
    "                score += (idf * df * numerator_constant) / (df + denominator_constant)\n",
    "        return score\n",
    "\n",
    "    def most_similar(self, index, n, include_self=False):\n",
    "        \"\"\"Computes and returns the highest BM25 scores for the given\n",
    "        index in the corpus in relation to all other documents in the corpus\n",
    "        Parameters\n",
    "        ----------\n",
    "        index: int\n",
    "            Index of the document to be scored.\n",
    "        n: int\n",
    "            Number of most similar scores to return\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        indexes = range(self.corpus_size)\n",
    "        score = lambda i: self._compare_documents(index, i)\n",
    "\n",
    "        if not include_self:\n",
    "            indexes = filter(lambda i: i != index, indexes)\n",
    "\n",
    "        return sorted(indexes, key=score, reverse=True)[:n]\n",
    "\n",
    "    def _compare_documents(self, index1, index2):\n",
    "        \"\"\"Computes and returns the BM25 score for a pair of documents\n",
    "        with the given indexes\n",
    "        Parameters\n",
    "        ----------\n",
    "        index1: int\n",
    "            Index of the document to be scored.\n",
    "        index2: int\n",
    "            Index of the other document to be scored against.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        doc1_freqs = self.doc_freqs[index1]\n",
    "        doc2_scores = self.doc_scores[index2]\n",
    "\n",
    "        in_common = doc1_freqs.keys() & doc2_scores.keys()\n",
    "\n",
    "        score = 0.0\n",
    "        for word in in_common:\n",
    "            score += doc2_scores[word] * doc1_freqs[word]\n",
    "        return score\n",
    "\n",
    "    def compare_documents(self, index, indexes):\n",
    "        \"\"\"Computes and returns BM25 scores of given index in the corpus\n",
    "        in relation to the given indexes\n",
    "        Parameters\n",
    "        ----------\n",
    "        index: int\n",
    "            Index of the document to be scored.\n",
    "        indexes: list of int\n",
    "            Indexes of the other documents used in scoring.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        return [self._compare_documents(index, i) for i in indexes]\n",
    "\n",
    "    def compare_all_documents(self, index):\n",
    "        \"\"\"Computes and returns BM25 scores of given index in the corpus\n",
    "        in relation to every other item in corpus.\n",
    "        Parameters\n",
    "        ----------\n",
    "        index: int\n",
    "            Index of the document to be scored.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        return self.compare_documents(index, range(self.corpus_size))\n",
    "\n",
    "    def get_scores(self, document):\n",
    "        \"\"\"Computes and returns BM25 scores of given `document` in relation to\n",
    "        every item in corpus.\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : list of str\n",
    "            Document to be scored.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        scores = [self.get_score(document, index) for index in range(self.corpus_size)]\n",
    "        return scores\n",
    "\n",
    "    def get_scores_bow(self, document):\n",
    "        \"\"\"Computes and returns BM25 scores of given `document` in relation to\n",
    "        every item in corpus.\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : list of str\n",
    "            Document to be scored.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for index in range(self.corpus_size):\n",
    "            score = self.get_score(document, index)\n",
    "            if score > 0:\n",
    "                scores.append((index, score))\n",
    "        return scores\n",
    "\n",
    "\n",
    "def _get_scores_bow(bm25, document):\n",
    "    \"\"\"Helper function for retrieving bm25 scores of given `document` in parallel\n",
    "    in relation to every item in corpus.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bm25 : BM25 object\n",
    "        BM25 object fitted on the corpus where documents are retrieved.\n",
    "    document : list of str\n",
    "        Document to be scored.\n",
    "    Returns\n",
    "    -------\n",
    "    list of (index, float)\n",
    "        BM25 scores in a bag of weights format.\n",
    "    \"\"\"\n",
    "    return bm25.get_scores_bow(document)\n",
    "\n",
    "\n",
    "def _get_scores(bm25, document):\n",
    "    \"\"\"Helper function for retrieving bm25 scores of given `document` in parallel\n",
    "    in relation to every item in corpus.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bm25 : BM25 object\n",
    "        BM25 object fitted on the corpus where documents are retrieved.\n",
    "    document : list of str\n",
    "        Document to be scored.\n",
    "    Returns\n",
    "    -------\n",
    "    list of float\n",
    "        BM25 scores.\n",
    "    \"\"\"\n",
    "    return bm25.get_scores(document)\n",
    "\n",
    "\n",
    "def iter_bm25_bow(corpus, n_jobs=1, k1=PARAM_K1, b=PARAM_B, epsilon=EPSILON):\n",
    "    \"\"\"Yield BM25 scores (weights) of documents in corpus.\n",
    "    Each document has to be weighted with every document in given corpus.\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of list of str\n",
    "        Corpus of documents.\n",
    "    n_jobs : int\n",
    "        The number of processes to use for computing bm25.\n",
    "    k1 : float\n",
    "        Constant used for influencing the term frequency saturation. After saturation is reached, additional\n",
    "        presence for the term adds a significantly less additional score. According to [1]_, experiments suggest\n",
    "        that 1.2 < k1 < 2 yields reasonably good results, although the optimal value depends on factors such as\n",
    "        the type of documents or queries.\n",
    "    b : float\n",
    "        Constant used for influencing the effects of different document lengths relative to average document length.\n",
    "        When b is bigger, lengthier documents (compared to average) have more impact on its effect. According to\n",
    "        [1]_, experiments suggest that 0.5 < b < 0.8 yields reasonably good results, although the optimal value\n",
    "        depends on factors such as the type of documents or queries.\n",
    "    epsilon : float\n",
    "        Constant used as floor value for idf of a document in the corpus. When epsilon is positive, it restricts\n",
    "        negative idf values. Negative idf implies that adding a very common term to a document penalize the overall\n",
    "        score (with 'very common' meaning that it is present in more than half of the documents). That can be\n",
    "        undesirable as it means that an identical document would score less than an almost identical one (by\n",
    "        removing the referred term). Increasing epsilon above 0 raises the sense of how rare a word has to be (among\n",
    "        different documents) to receive an extra score.\n",
    "    Yields\n",
    "    -------\n",
    "    list of (index, float)\n",
    "        BM25 scores in bag of weights format.\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "        >>> from gensim.summarization.bm25 import iter_bm25_weights\n",
    "        >>> corpus = [\n",
    "        ...     [\"black\", \"cat\", \"white\", \"cat\"],\n",
    "        ...     [\"cat\", \"outer\", \"space\"],\n",
    "        ...     [\"wag\", \"dog\"]\n",
    "        ... ]\n",
    "        >>> result = iter_bm25_weights(corpus, n_jobs=-1)\n",
    "    \"\"\"\n",
    "    bm25 = BM25(corpus, k1, b, epsilon)\n",
    "\n",
    "    n_processes = effective_n_jobs(n_jobs)\n",
    "    if n_processes == 1:\n",
    "        for doc in corpus:\n",
    "            yield bm25.get_scores_bow(doc)\n",
    "        return\n",
    "\n",
    "    get_score = partial(_get_scores_bow, bm25)\n",
    "    pool = Pool(n_processes)\n",
    "\n",
    "    for bow in pool.imap(get_score, corpus):\n",
    "        yield bow\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "def get_bm25_weights(corpus, n_jobs=1, k1=PARAM_K1, b=PARAM_B, epsilon=EPSILON):\n",
    "    \"\"\"Returns BM25 scores (weights) of documents in corpus.\n",
    "    Each document has to be weighted with every document in given corpus.\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of list of str\n",
    "        Corpus of documents.\n",
    "    n_jobs : int\n",
    "        The number of processes to use for computing bm25.\n",
    "    k1 : float\n",
    "        Constant used for influencing the term frequency saturation. After saturation is reached, additional\n",
    "        presence for the term adds a significantly less additional score. According to [1]_, experiments suggest\n",
    "        that 1.2 < k1 < 2 yields reasonably good results, although the optimal value depends on factors such as\n",
    "        the type of documents or queries.\n",
    "    b : float\n",
    "        Constant used for influencing the effects of different document lengths relative to average document length.\n",
    "        When b is bigger, lengthier documents (compared to average) have more impact on its effect. According to\n",
    "        [1]_, experiments suggest that 0.5 < b < 0.8 yields reasonably good results, although the optimal value\n",
    "        depends on factors such as the type of documents or queries.\n",
    "    epsilon : float\n",
    "        Constant used as floor value for idf of a document in the corpus. When epsilon is positive, it restricts\n",
    "        negative idf values. Negative idf implies that adding a very common term to a document penalize the overall\n",
    "        score (with 'very common' meaning that it is present in more than half of the documents). That can be\n",
    "        undesirable as it means that an identical document would score less than an almost identical one (by\n",
    "        removing the referred term). Increasing epsilon above 0 raises the sense of how rare a word has to be (among\n",
    "        different documents) to receive an extra score.\n",
    "    Returns\n",
    "    -------\n",
    "    list of list of float\n",
    "        BM25 scores.\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "        >>> from gensim.summarization.bm25 import get_bm25_weights\n",
    "        >>> corpus = [\n",
    "        ...     [\"black\", \"cat\", \"white\", \"cat\"],\n",
    "        ...     [\"cat\", \"outer\", \"space\"],\n",
    "        ...     [\"wag\", \"dog\"]\n",
    "        ... ]\n",
    "        >>> result = get_bm25_weights(corpus, n_jobs=-1)\n",
    "    \"\"\"\n",
    "    bm25 = BM25(corpus, k1, b, epsilon)\n",
    "\n",
    "    n_processes = effective_n_jobs(n_jobs)\n",
    "    if n_processes == 1:\n",
    "        weights = [bm25.get_scores(doc) for doc in corpus]\n",
    "        return weights\n",
    "\n",
    "    get_score = partial(_get_scores, bm25)\n",
    "    pool = Pool(n_processes)\n",
    "    weights = pool.map(get_score, corpus)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2db2034",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27990/2285984060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msearch_n_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiple_k_scorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from . import get_X_y\n",
    "from ..consts import search_n_iters\n",
    "from .scoring import multiple_k_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def get_folds(df, n):\n",
    "    \"\"\"Creates custom folds to be used in hyperparameter tuning\n",
    "    These folds guarantee that no test dups are contained in the train\n",
    "    folds\n",
    "\n",
    "    If we use sklearn's default algorithm, it would randomly select pairs\n",
    "    and the test dups have pairs in the train set, which could lead to\n",
    "    leakage\n",
    "    \"\"\"\n",
    "    dups = df[df.is_dup][[\"dup_id\"]]\n",
    "    dups = dups.drop_duplicates()\n",
    "\n",
    "    # shuffle dups\n",
    "    dups = dups.sample(frac=1, random_state=42)\n",
    "    dups = dups.dup_id\n",
    "\n",
    "    folds = []\n",
    "    for test_dups in np.array_split(dups, n):\n",
    "        test_ids = df[df.dup_id.isin(test_dups)]\n",
    "        test_ids = test_ids.index\n",
    "\n",
    "        train_ids = df[~df.dup_id.isin(test_dups)]\n",
    "        train_ids = train_ids.index\n",
    "\n",
    "        folds.append((train_ids, test_ids))\n",
    "    return folds\n",
    "\n",
    "\n",
    "def random_forest_tuner(folds):\n",
    "    \"\"\"Returns an instance of a RandomizedSearchCV with pre-defined\n",
    "    parameters for a random forest\n",
    "    \"\"\"\n",
    "    # parameters used for tuning random forests\n",
    "    randomforest_hp_grid = {\n",
    "        \"n_estimators\": [int(x) for x in np.linspace(50, 1000, num=50)],\n",
    "        \"max_depth\": [int(x) for x in np.linspace(5, 100, num=50)],\n",
    "        \"min_samples_split\": [2, 3, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "        \"random_state\": [42],\n",
    "    }\n",
    "\n",
    "    est = RandomForestClassifier()\n",
    "    return RandomizedSearchCV(\n",
    "        est,\n",
    "        randomforest_hp_grid,\n",
    "        n_iter=search_n_iters,\n",
    "        cv=folds,\n",
    "        scoring=multiple_k_scorer,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=7,\n",
    "        refit=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def tune_train_set(train, n):\n",
    "    \"\"\"Tunes hyperparameters using the provided train set\"\"\"\n",
    "    X, y = get_X_y(train)\n",
    "    folds = get_folds(train, n)\n",
    "    rs = random_forest_tuner(folds)\n",
    "\n",
    "    return rs.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575ec87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(df):\n",
    "    \"\"\"Creates the X (features) and y (target) datasets\n",
    "    to be used by the classifier\n",
    "    \"\"\"\n",
    "    X = df.set_index([\"candidate_id\", \"dup_id\"])\n",
    "    y = X[\"is_dup\"]\n",
    "    X = X.drop(columns=[\"is_dup\", \"score\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17a4575",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27990/849578130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbm25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBM25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mn_procs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from .bm25 import BM25\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from ..consts import n_procs\n",
    "\n",
    "\n",
    "def doc2vec_model(corpus_path):\n",
    "    \"\"\"Defines a Doc2Vec model with predefined parameters\"\"\"\n",
    "    return Doc2Vec(\n",
    "        corpus_file=str(corpus_path),\n",
    "        vector_size=30,\n",
    "        window=15,\n",
    "        min_count=5,\n",
    "        workers=n_procs,\n",
    "        seed=42,\n",
    "        sample=1e-5,\n",
    "        negative=1,\n",
    "        epochs=25,\n",
    "    )\n",
    "\n",
    "\n",
    "def bm25_model(corpus):\n",
    "    \"\"\"Defines a BM25 model with predefined parameters\"\"\"\n",
    "    return BM25(corpus, k1=0.05, b=0.03)\n",
    "\n",
    "\n",
    "def lda_model(corpus, vocab):\n",
    "    \"\"\"Defines an LDA model with predefined parameters\"\"\"\n",
    "    return LdaMulticore(\n",
    "        corpus,\n",
    "        random_state=42,\n",
    "        id2word=vocab,\n",
    "        alpha=\"symmetric\",\n",
    "        eta=\"auto\",\n",
    "        eval_every=5,\n",
    "        num_topics=30,\n",
    "        workers=n_procs,\n",
    "        minimum_probability=0.0,\n",
    "    )\n",
    "\n",
    "\n",
    "def bertoverflow_model():\n",
    "    \"\"\"Loads the BERTOverflow model and converts it to a sentence transformer\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "    bertoverflow = models.Transformer(\"jeniya/BERTOverflow\")\n",
    "    pooling_model = models.Pooling(bertoverflow.get_word_embedding_dimension())\n",
    "    return SentenceTransformer(modules=[bertoverflow, pooling_model])\n",
    "\n",
    "\n",
    "def mpnet_model():\n",
    "    \"\"\"Loads the MPNet model\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    return SentenceTransformer(\"paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "049191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_rate_k_scorer(df, col, k):\n",
    "    \"\"\"Scores a dataframe based on the column using the recall-rate@k measure\"\"\"\n",
    "\n",
    "    def recall_rate_k(df):\n",
    "        return df.sort_values(col, ascending=False)[:k][\"is_dup\"].any()\n",
    "\n",
    "    return df.groupby(\"dup_id\").apply(recall_rate_k).mean()\n",
    "\n",
    "\n",
    "def predict_probabilities(estimator, X, y):\n",
    "    \"\"\"Uses the classifier to predict the classification probabilities using the set of features X\"\"\"\n",
    "    df = X.reset_index()[[\"dup_id\", \"candidate_id\"]]\n",
    "    df = df.set_index([\"candidate_id\", \"dup_id\"])\n",
    "\n",
    "    df[\"pred\"] = [p[1] for p in estimator.predict_proba(X)]\n",
    "    df[\"is_dup\"] = y\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def multiple_k_scorer(estimator, X, y):\n",
    "    \"\"\"Uses the estimator to predict values of X, and evaluates it using multiple\n",
    "    recall-rates measures\n",
    "    \"\"\"\n",
    "    df = predict_probabilities(estimator, X, y)\n",
    "\n",
    "    return {\n",
    "        \"rr@5\": recall_rate_k_scorer(df, \"pred\", 5),\n",
    "        \"rr@10\": recall_rate_k_scorer(df, \"pred\", 10),\n",
    "        \"rr@20\": recall_rate_k_scorer(df, \"pred\", 20),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb2bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing scripts from the other dir\n",
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.getcwd())\n",
    "scripts = path.rsplit('code', 1)[0] + 'code/scripts'\n",
    "sys.path.insert(0, scripts)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import read, paths\n",
    "from utils.consts import datasets, gamedev_datasets\n",
    "\n",
    "def dataset_name(ds):\n",
    "    \"\"\"Returns the pretty version of the dataset name based\n",
    "    on the string that represents it\n",
    "    \"\"\"\n",
    "    if ds == 'gamedev_se':\n",
    "        name = 'Game Dev. Stack Exchange'\n",
    "    elif ds == 'gamedev_so':\n",
    "        name = 'Stack Overflow (Game dev.)'\n",
    "    else:\n",
    "        name = 'Stack Overflow (General dev.)'\n",
    "    return name\n",
    "\n",
    "def dup_pair_similarity_summary():\n",
    "    \"\"\"Creates a summary table with all the recall-rates for all similarity scores\n",
    "    and formats it\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    for ds in datasets:\n",
    "        df = read(paths.all_pair_ranks(ds))\n",
    "    \n",
    "        sim_names = {\n",
    "            \"jaccard\": \"Jaccard\",\n",
    "            \"bm25\": \"BM25\",\n",
    "            \"tfidf\": \"TF-IDF\",\n",
    "            \"doc2vec\": \"Doc2Vec\",\n",
    "            \"topic\": \"Topic\",\n",
    "            \"bertoverflow\": \"BERTOverflow\",\n",
    "            \"mpnet\": \"MPNet\",\n",
    "        }\n",
    "    \n",
    "        # order in which to show results\n",
    "        score_order = [\"recall-rate@5\", \"recall-rate@10\", \"recall-rate@20\"]\n",
    "        doc_order = [\n",
    "            \"title\",\n",
    "            \"body\",\n",
    "            \"tags\",\n",
    "            \"title_body\",\n",
    "            \"title_body_tags\",\n",
    "            \"title_body_tags_answer\",\n",
    "        ]\n",
    "        sim_order = [\n",
    "            \"Jaccard\",\n",
    "            \"TF-IDF\",\n",
    "            \"BM25\",\n",
    "            \"Topic\",\n",
    "            \"Doc2Vec\",\n",
    "            \"BERTOverflow\",\n",
    "            \"MPNet\",\n",
    "        ]\n",
    "    \n",
    "        # rename features\n",
    "        df.feature = df.feature.apply(lambda f: sim_names[f])\n",
    "    \n",
    "        df = df.pivot_table(values=score_order, columns=\"feature\", index=\"col\")\n",
    "        df = df.transpose()\n",
    "        df.columns = list(df.columns)\n",
    "    \n",
    "        df.index = df.index.set_names([\"Score\", \"Similarity\"])\n",
    "        df = df[doc_order]\n",
    "        # change column names to fit page\n",
    "        df.columns = [\"(1)\", \"(2)\", \"(3)\", \"(4)\", \"(5)\", \"(6)\"]\n",
    "        df = df.reindex(sim_order, level=1)\n",
    "        df = df.reindex(score_order, level=0)\n",
    "        # turn recall-rates into percentages with 2 decimals\n",
    "        df = (df * 100).round(2)\n",
    "        \n",
    "        df[\"Dataset\"] = dataset_name(ds)\n",
    "    \n",
    "        summary.append(df)\n",
    "    \n",
    "    summary = pd.concat(summary)\n",
    "    summary = summary.reset_index()\n",
    "    \n",
    "    # calculate std and mean\n",
    "    summary = summary.groupby(['Dataset', 'Score', 'Similarity']).agg([np.mean, np.std])\n",
    "    \n",
    "    # format values\n",
    "    for c in set(summary.columns.get_level_values(0)):\n",
    "        summary[(c, 'mean')] = summary[(c, 'mean')].apply(lambda f: \"{:.2f}\".format(f))\n",
    "        summary[(c, 'std')] = summary[(c, 'std')].apply(lambda f: \"\" if pd.isna(f) else \"({:.2f})\".format(round(f, 2)))\n",
    "        summary[(c, 'mean')] = summary[(c, 'mean')] + \" \" + summary[(c, 'std')]\n",
    "        \n",
    "    # drop columns + fix order\n",
    "    summary = summary.iloc[:,summary.columns.get_level_values(1) == 'mean']\n",
    "    summary.columns = summary.columns.droplevel(1)\n",
    "    summary = summary.reindex(sim_order, level=2)\n",
    "    summary = summary.reindex(score_order, level=1)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def candidate_evaluation_summary():\n",
    "    \"\"\"Formats the dataframe containing the evaluation results for different numbers of candidates\"\"\"\n",
    "    df = read(paths.candidates_evaluation())\n",
    "    df['dataset'] = df['dataset_train'].apply(dataset_name)\n",
    "    \n",
    "    df = df.drop(columns='dataset_train')\n",
    "    \n",
    "    for c in [c for c in df.columns if 'rr' in c]:\n",
    "        df[c] = (df[c]*100).round(2)\n",
    "        \n",
    "    df = df.groupby(['dataset', 'candidates']).agg([np.mean, np.std])\n",
    "    \n",
    "    # format values\n",
    "    for c in set(df.columns.get_level_values(0)):\n",
    "        df[(c, 'mean')] = df[(c, 'mean')].apply(lambda f: \"{:.2f}\".format(f))\n",
    "        df[(c, 'std')] = df[(c, 'std')].apply(lambda f: \"\" if pd.isna(f) else \"({:.2f})\".format(round(f, 2)))\n",
    "        df[(c, 'mean')] = df[(c, 'mean')] + \" \" + df[(c, 'std')]\n",
    "        \n",
    "    # drop columns + fix order and naming\n",
    "    df = df.iloc[:,df.columns.get_level_values(1) == 'mean']\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df.columns = pd.MultiIndex.from_product([['Recall-rate@'], [c[3:] for c in df.columns]])\n",
    "    return df\n",
    "\n",
    "def dataset_sizes():\n",
    "    \"\"\"Summarizes the sizes of the datasets used in the study\"\"\"\n",
    "    df = []\n",
    "    for ds in datasets:\n",
    "        n_qs = len(read(paths.all_question_ids(ds)))\n",
    "        pairs = read(paths.dup_pairs(ds))\n",
    "        n_dups = len(read(paths.duplicate_question_ids(ds)))\n",
    "        \n",
    "        if ds == 'gamedev_se':\n",
    "            source = 'Stack Exchange'\n",
    "        else:\n",
    "            source = 'Stack Overflow'\n",
    "            \n",
    "        if 'so_samples' not in ds:\n",
    "            topic = 'Game development'\n",
    "        else:\n",
    "            topic = 'General development'\n",
    "        \n",
    "        df.append({\n",
    "            'Source': source,\n",
    "            'Topic': topic,\n",
    "            'Questions': n_qs,\n",
    "            'Non-duplicates': n_qs-n_dups,\n",
    "            'Duplicates': n_dups,\n",
    "            'Pairs': len(pairs)\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.groupby(['Source', 'Topic']).agg([np.mean, np.std])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    if (df.iloc[:,df.columns.get_level_values(1) == 'std'] == 0).all().all():\n",
    "        print('All datasets have std == 0')\n",
    "    \n",
    "    df = df.iloc[:,df.columns.get_level_values(1) == 'mean']\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df['dup_perc'] = df['Duplicates']/df['Questions']*100\n",
    "    df['dup_perc'] = df['dup_perc'].apply(lambda x: round(x, 1))\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if c != 'dup_perc':\n",
    "            df[c] = df[c].apply(lambda i: \"{:,}\".format(i))\n",
    "            \n",
    "    df['Duplicates'] = df['Duplicates'] + ' (' + df['dup_perc'].apply(str) + '%)'\n",
    "    df = df.drop(columns='dup_perc')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def cross_dataset_summary():\n",
    "    \"\"\"Creates a table summarizing the performance of the classifiers\n",
    "    when testing on other datasets\n",
    "    \"\"\"\n",
    "    df = read(paths.cross_dataset_performance())\n",
    "    df['dataset_train'] = df['dataset_train'].apply(dataset_name)\n",
    "    df['dataset_test'] = df['dataset_test'].apply(dataset_name)\n",
    "\n",
    "    for c in [c for c in df.columns if 'rr' in c]:\n",
    "        df[c] = (df[c]*100).round(2)\n",
    "\n",
    "    df = df.groupby(['dataset_train', 'dataset_test']).agg([np.mean, np.std])\n",
    "\n",
    "    # format values\n",
    "    for c in set(df.columns.get_level_values(0)):\n",
    "        df[(c, 'mean')] = df[(c, 'mean')].apply(lambda f: \"{:.2f}\".format(f))\n",
    "        df[(c, 'std')] = df[(c, 'std')].apply(lambda f: \"\" if pd.isna(f) else \"({:.2f})\".format(round(f, 2)))\n",
    "        df[(c, 'mean')] = df[(c, 'mean')] + \" \" + df[(c, 'std')]\n",
    "\n",
    "    # drop columns + fix order and naming\n",
    "    df = df.iloc[:,df.columns.get_level_values(1) == 'mean']\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df = df.sort_index(level=1)\n",
    "    df.columns = pd.MultiIndex.from_product([['Recall-rate@'], [c[3:] for c in df.columns]])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def misclassified_summary():\n",
    "    \"\"\"Creates a table summarizing the duplicates that were misclassified by our classifiers\"\"\"\n",
    "    def to_percentage(a, b):\n",
    "        p = round(a/b*100)\n",
    "        return f'({p}%)'\n",
    "    \n",
    "    df = {}\n",
    "    \n",
    "    df['Description'] = [\n",
    "        'Duplicate pairs in test set',\n",
    "        'Misclassified duplicate pairs',\n",
    "        'Main question not in the list of candidates',\n",
    "        'Top ranked question is an unlabelled duplicate',\n",
    "        'Main question discusses a more general topic'\n",
    "    ]\n",
    "    \n",
    "    for ds in gamedev_datasets + ['so_samples/sample_0']:\n",
    "        test = read(paths.test_set(ds))\n",
    "        dups_in_test = len(test.dup_id.unique())\n",
    "        \n",
    "        if 'so_samples' in ds:\n",
    "            missed = read(paths.misclassified_duplicates('so_sample'))\n",
    "        else:\n",
    "            missed = read(paths.misclassified_duplicates(ds))\n",
    "        \n",
    "        n_misclassified = len(missed)\n",
    "        no_candidate = len(missed[~missed.has_dup])\n",
    "        top_ranked_dup = len(missed[missed.top_ranked_is_dup])\n",
    "        main_more_general = len(missed[missed.main_more_generic])\n",
    "        \n",
    "        misc_perc = to_percentage(n_misclassified, n_misclassified)\n",
    "        no_candidate_perc = to_percentage(no_candidate, n_misclassified)\n",
    "        top_ranked_perc = to_percentage(top_ranked_dup, n_misclassified)\n",
    "        more_general_perc = to_percentage(main_more_general, n_misclassified)\n",
    "    \n",
    "        n_misclassified = str(n_misclassified) + ' ' + misc_perc\n",
    "        no_candidate = str(no_candidate) + ' ' + no_candidate_perc\n",
    "        top_ranked_dup = str(top_ranked_dup) + ' ' + top_ranked_perc\n",
    "        main_more_general = str(main_more_general) + ' ' + more_general_perc\n",
    "        \n",
    "        ds = dataset_name(ds)\n",
    "        \n",
    "        df[ds] = [dups_in_test, n_misclassified, no_candidate, top_ranked_dup, main_more_general]\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce4e74a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'create_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27990/3690282884.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcreate_tables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_rows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'create_tables'"
     ]
    }
   ],
   "source": [
    "from create_tables import *\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722be17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
